{"cells":[{"cell_type":"markdown","source":["#                                     Food-disease-outbreaks prediction"],"metadata":{}},{"cell_type":"markdown","source":["## About : \n### One in every person in the United States gets sick from eating food at various location in different states. While most foodborne illnesses are not considered outbreaks. But if some of them are at a larger scale, they can be life threatening. The problem at hand is the Foodborne outbreaks in the past caused by various factors.  We want to find a way to predict the major factors that contribute towards foodborne diseases."],"metadata":{}},{"cell_type":"markdown","source":["## Features :\n### 1.Supervised Learning System — It takes into consideration the key features like State, Month, Year, Food Consumed, Location of Food Consumption to predict the possible number of illnesses. This is surely helpful for Centre for Disease Control and Prevention to taking neccessary measures ahead of time to mitigate the risks.\n\n### 2.Type of Predictions Made\n\n   #### Prediction of Illnesses Count based on the features mentioned above\n   #### Classification of Diseases as High Scaled or Low Scaled\n   #### Time Series Analysis to understand the impact of Month and Year on the Oubtreak Impact\n\n### 3.Algorithms Used — There are multiple algorithms involved in this system to tackle the outbreaks.\n\n   #### Regression Models - Linear Regression, Linear Regression with Regularization and Cross Validation, Decision Tree Regression, Random Forest Regression\n   #### Classfication Model- Logistic Regression\n   #### Time Series Analysis - Base Time Series Model, Rolling Average Model\n\n### 4.Data Cleansing and Normalization — Data is messy and hence we have perfomed data cleaning using pandas to ensure that data loss is minimal and the values which we are predicted are uniformly distributed to ensure best outcomes. Example of operations - Removing null values, replacing null values with mean values, removing non impactful features, taking Logarithm of prediction values to bring it on a normalized scale.\n\n### 5.Data Visualization to represent findings — At many instances, the data has been represented using Matplotlib, Seaborn and Pandas library to assist stakeholders in understanding the findings in the easist and user friendly way.\n\n### 6.String Indexing, One Hot Encoding - String Indexing and One hot encoding is performed\n\n### 7.Evaluation criteria \n   #### Regression models - RMSE\n   #### Classification models - AUC ROC"],"metadata":{}},{"cell_type":"markdown","source":["### Performing Machine Learning on a Food-based illness dataset available from CDC and FDA to predict illness counts based on State, Month, Location and type of Food Consumed"],"metadata":{}},{"cell_type":"markdown","source":["### The dataset used for the propect is the Foodborne Disease Outbreaks, 1998-2015."],"metadata":{}},{"cell_type":"markdown","source":["### Link to the dataset - https://www.kaggle.com/cdc/foodborne-diseases"],"metadata":{}},{"cell_type":"markdown","source":["## Prerequisites \n### 1. Databricks community edition (alternatively Jupyter notebook installed on your local machine)\n### 2. Python version 3.6.1\n### 3. Spark version 2.0\n### 4. Libraries - pandas, numpy, pyspark, matplotilib, seaborn\n### 5. Python package manager 'PIP'"],"metadata":{}},{"cell_type":"markdown","source":["## Contributors:\n\n### 1. Anmol Handa\n### 2. Anuj Jain\n### 3. Justin Thierry"],"metadata":{}},{"cell_type":"markdown","source":["## Special Thanks:\n### 1. Dr. Daniel Acuna\n### 2. Mr. Tong Zeng"],"metadata":{}},{"cell_type":"markdown","source":["## Getting setup:\n### 1. Local machine\n###  - Installing Python version 3.6.1 on local machine\n###  - Installing 'PIP', the package manager for python\n###  - Installing the libraries pandas, numpy, matplotlib, pyspark, seaborn ( ex. pip install pandas ) from cmd\n###  - Installing jupyter notebook ( cmd - pip install jupyter ) from cmd\n###  - Launching jupyter notebook ( cmd - jupyter notebook ) from cmd\n###  - Putting the csv/dataset in the working directory\n\n### 2. Databricks\n###  - Picking Python version 3.6.1\n###  - Creating a spark cluster\n###  - Putting the csv/dataset in the HDFS directory of databricks"],"metadata":{}},{"cell_type":"markdown","source":["## For further reference to the code, following is the github link\n## https://github.com/handaanmol/Food-disease-outbreaks"],"metadata":{}},{"cell_type":"markdown","source":["## Beginning of the code"],"metadata":{}},{"cell_type":"markdown","source":["#### The following python libraries are imported to load various utility, mathematical, visualization, pipelines and machine learning packages in python."],"metadata":{}},{"cell_type":"code","source":["#linear algebra and mathematical packages\nimport numpy as np \nimport pandas as pd \n\n#visualization packages\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#spark 2 related packages like SQL, machine learning\nfrom pyspark.sql.types import *\nimport pyspark.sql.functions as F\nfrom pyspark.sql.functions import col, udf\n\nfrom pyspark.ml.stat import Correlation\n\nfrom pyspark.ml.classification import LogisticRegression\n\nfrom pyspark.ml.tuning import ParamGridBuilder, CrossValidator, CrossValidatorModel\n\nfrom pyspark.ml.feature import Bucketizer, StringIndexer, OneHotEncoder, StandardScaler, VectorAssembler\n\nfrom pyspark.ml import Pipeline, PipelineModel\nfrom pyspark.ml.tuning import ParamGridBuilder, CrossValidator, CrossValidatorModel\n\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\nfrom pyspark.mllib.evaluation import MulticlassMetrics\nfrom pyspark.ml.evaluation import RegressionEvaluator\n%matplotlib inline\n"],"metadata":{"collapsed":true},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":["#### In the following code, we are importing the dataset from the our machine into databricks cluster. Further, we are reading that data on the databricks cluster"],"metadata":{}},{"cell_type":"markdown","source":["#### To import dataset in local machine, you need to change the path to the local path**"],"metadata":{}},{"cell_type":"code","source":["outbreaks = pd.read_csv(\"/dbfs/FileStore/tables/outbreaks.csv\")\noutbreaks.head(10)"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":["#### Checking the null values for all columns in the comma seperated values dataset."],"metadata":{}},{"cell_type":"markdown","source":["#### We found that null values are high in the columns names 'ingredients' and 'serotype/genotype'"],"metadata":{}},{"cell_type":"code","source":["outbreaks.isnull().sum()"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":["#### There are 19119 records in the dataset, and the columns 'ingredients' and 'serotype' have almost the same number of null values. So we will not be considering these columns as part of our analysis. Following is a heatmap that shows the distribution of null values in the dataset. Yellow is represented as null values."],"metadata":{}},{"cell_type":"code","source":["plt.cla()\nsns.heatmap(outbreaks.isnull(), yticklabels=False, cbar=False, cmap='viridis')\nplt.tight_layout()\ndisplay()"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":["#### Renaming the column \"Serotype/Genotype\" to \"Serotype\""],"metadata":{}},{"cell_type":"code","source":["outbreaks =outbreaks.rename(index=str, columns={\"Serotype/Genotype\": \"Serotype\"})"],"metadata":{"collapsed":true},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":["#### Plotting the distribution of the values under the column \"Species\""],"metadata":{}},{"cell_type":"code","source":["plt.subplots(figsize=(20,15))\nsns.countplot(x='Species', data=outbreaks, order= outbreaks.Species.value_counts().iloc[2:8].index)\ndisplay()"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":["#### Plotting the distribution of the illnesses throughout all the states in the U.S.A"],"metadata":{}},{"cell_type":"code","source":["plt.cla()\ndf2 = pd.pivot_table(outbreaks, index='State', values='Illnesses', aggfunc='count')\nax = df2.plot(kind='bar', color='steelblue',figsize=(25,10))\nplt.title('Foodborne Illnesses Cases By Year')\nplt.ylabel('Illiness Cases')\ndisplay()"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"markdown","source":["#### Showing the distribution of food types in the data. As we see below, these are some of the most popular food types consumed."],"metadata":{}},{"cell_type":"code","source":["outbreaks.Food.value_counts()"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"markdown","source":["#### Dropping columns from the dataset that we will not use for analysis. The columns are \"Ingredient\",\"Serotype\",\"Species\",\"Status\",\"Fatalities\""],"metadata":{}},{"cell_type":"code","source":["outbreaks.drop(['Ingredient', 'Serotype', 'Species', 'Status', 'Fatalities'], axis=1, inplace=True)\noutbreaks.head()"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"markdown","source":["#### Plotting the new reduced dataset to see the frequency of null values"],"metadata":{}},{"cell_type":"code","source":["plt.cla()\nsns.heatmap(outbreaks.isnull(), yticklabels=False, cbar=False, cmap='viridis')\ndisplay()"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"markdown","source":["#### Checking Null values in location"],"metadata":{}},{"cell_type":"code","source":["outbreaks.isnull().sum()"],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"markdown","source":["#### Checking count of values in dataset"],"metadata":{}},{"cell_type":"code","source":["outbreaks.count()"],"metadata":{},"outputs":[],"execution_count":38},{"cell_type":"markdown","source":["#### Plotting how illnesses are distributed over the dataset"],"metadata":{}},{"cell_type":"code","source":["plt.cla()\nsns.distplot(outbreaks.Illnesses, bins=10, color='red')\nplt.title('Distribution of Illnesses in Traning Set')\ndisplay()"],"metadata":{},"outputs":[],"execution_count":40},{"cell_type":"markdown","source":["#### Plotting the Distribution of FoodBorne Illnesses by State. We found that California and illinois are the states with the most illnesses."],"metadata":{}},{"cell_type":"code","source":["plt.cla()\ndf2 = pd.pivot_table(outbreaks, index='State', values='Illnesses', aggfunc='sum')\nax = df2.plot(kind='bar', color='steelblue',figsize=(25,10))\nplt.title('Foodborne Illnesses Cases By State')\nplt.ylabel('Illiness Cases')\ndisplay()"],"metadata":{},"outputs":[],"execution_count":42},{"cell_type":"markdown","source":["#### Plotting the Distribution of FoodBorne Illnesses by Year. We found that there is declining trend with some crests and troughs."],"metadata":{}},{"cell_type":"code","source":["plt.cla()\ndf2 = pd.pivot_table(outbreaks, index='Year', values='Illnesses', aggfunc='sum')\nax = df2.plot(kind='bar', color='steelblue',figsize=(25,10))\nplt.title('Foodborne Illnesses Cases By Year')\nplt.ylabel('Illness Cases')\ndisplay()"],"metadata":{"scrolled":true},"outputs":[],"execution_count":44},{"cell_type":"markdown","source":["#### Plotting the Distribution of FoodBorne Illnesses by Months. There is no interesting trend found."],"metadata":{}},{"cell_type":"code","source":["#Distribution of illness by Months\nplt.cla()\ndf2 = pd.pivot_table(outbreaks, index='Month', values='Illnesses', aggfunc='mean')\nax = df2.plot(kind='bar', color='steelblue',figsize=(25,10))\nplt.title('Foodborne Illnesses Cases By Month')\nplt.ylabel('Illiness Cases')\ndisplay()"],"metadata":{},"outputs":[],"execution_count":46},{"cell_type":"markdown","source":["#### Looking at the number of top food items"],"metadata":{}},{"cell_type":"code","source":["outbreaks.Food.value_counts()"],"metadata":{},"outputs":[],"execution_count":48},{"cell_type":"markdown","source":["#### Replacing null values of food column with \"Unspecified\", and replacing null values in Location with \"Unknown\" to prevent data loss"],"metadata":{"collapsed":true}},{"cell_type":"code","source":["outbreaks.Food.fillna(\"Unspecified\", inplace=True)\noutbreaks.Location.fillna(\"Unknown\", inplace=True)\noutbreaks.Location.value_counts()\n"],"metadata":{},"outputs":[],"execution_count":50},{"cell_type":"markdown","source":["#### Filling Hospitalizations Null values with 0"],"metadata":{}},{"cell_type":"code","source":["outbreaks.Hospitalizations.fillna(0, inplace=True)"],"metadata":{"collapsed":true},"outputs":[],"execution_count":52},{"cell_type":"markdown","source":["#### Creating Normalized Column for Hospitalizations/Illnesses"],"metadata":{"collapsed":true}},{"cell_type":"code","source":["outbreaks['normalized_hospitalizations'] = outbreaks.apply(lambda row: round((row.Hospitalizations/row.Illnesses)*100), axis=1)"],"metadata":{"collapsed":true},"outputs":[],"execution_count":54},{"cell_type":"markdown","source":["#### Checking distrivution Normalized Hospitalizations over the dataset"],"metadata":{}},{"cell_type":"code","source":["plt.cla()\nsns.distplot(outbreaks.normalized_hospitalizations, bins=10, color='red')\nplt.title('Distribution of Normalized Hospitalizations')\ndisplay()"],"metadata":{},"outputs":[],"execution_count":56},{"cell_type":"code","source":["outbreaks.head(5)"],"metadata":{},"outputs":[],"execution_count":57},{"cell_type":"markdown","source":["#### Plotting the distribution of illnesses in the dataset to visualize its left skewness"],"metadata":{}},{"cell_type":"code","source":["plt.cla()\nsns.distplot(outbreaks.Illnesses, bins=10, color='red')\nplt.title('Distribution of Illnesses')\ndisplay()"],"metadata":{"scrolled":true},"outputs":[],"execution_count":59},{"cell_type":"markdown","source":["#### Plotting the distribution of illnesses in the dataset after they have been standardised by log scale"],"metadata":{}},{"cell_type":"code","source":["plt.cla()\nsns.distplot(np.log10(outbreaks.Illnesses), bins=10, color='red')\nplt.title('Distribution of Illnesses standardized by log scale')\ndisplay()"],"metadata":{},"outputs":[],"execution_count":61},{"cell_type":"markdown","source":["#### Adding new column log- illness in our data"],"metadata":{"collapsed":true}},{"cell_type":"code","source":["outbreaks['Illnesses_log'] = np.log(outbreaks.Illnesses)"],"metadata":{"collapsed":true},"outputs":[],"execution_count":63},{"cell_type":"code","source":["outbreaks.head()"],"metadata":{"scrolled":true},"outputs":[],"execution_count":64},{"cell_type":"markdown","source":["#### Loading libraries to make use of packages for dataframe manipulation, running regression and classification models"],"metadata":{}},{"cell_type":"code","source":["from pyspark.context import SparkContext\nfrom pyspark.sql.session import SparkSession\nfrom pyspark.sql import SQLContext\ns1 = SQLContext(sc)\n\nfrom pyspark.sql import functions as fn\n# Functionality for computing features\nfrom pyspark.ml import feature\n# Functionality for regression\nfrom pyspark.ml import regression\n# Funcionality for classification\nfrom pyspark.ml import classification\n# Object for creating sequences of transformations\nfrom pyspark.ml import Pipeline"],"metadata":{},"outputs":[],"execution_count":66},{"cell_type":"code","source":["df = spark.createDataFrame(outbreaks)"],"metadata":{},"outputs":[],"execution_count":67},{"cell_type":"code","source":["df.dtypes"],"metadata":{},"outputs":[],"execution_count":68},{"cell_type":"markdown","source":["#### Deciding the split ratio of the data and transforming them into training, testing and validation using randomsplit function"],"metadata":{}},{"cell_type":"code","source":["training_df, validation_df, testing_df = df.randomSplit([0.6, 0.3, 0.1])\ndisplay(training_df)"],"metadata":{},"outputs":[],"execution_count":70},{"cell_type":"markdown","source":["#### Base model using only \"Year\" as the feature"],"metadata":{}},{"cell_type":"code","source":["#Base Model\nmodel1 = Pipeline(stages=[\n  feature.VectorAssembler(inputCols=['Year'], outputCol='features'),\n  regression.LinearRegression(featuresCol='features', labelCol='Illnesses_log')  \n]).fit(training_df)"],"metadata":{},"outputs":[],"execution_count":72},{"cell_type":"code","source":["rmse = fn.sqrt(fn.avg((fn.col('Illnesses_log') - fn.col('prediction'))**2))\nmodel1.transform(validation_df).select(rmse).show()"],"metadata":{},"outputs":[],"execution_count":73},{"cell_type":"code","source":["model1.transform(testing_df).select(rmse).show(100)"],"metadata":{},"outputs":[],"execution_count":74},{"cell_type":"code","source":["model1.transform(testing_df).select((fn.col('Illnesses_log') - fn.col('prediction'))**2, fn.col('Illnesses_log'), fn.col('prediction')).show(5000)"],"metadata":{},"outputs":[],"execution_count":75},{"cell_type":"markdown","source":["#### We found that RMSE values of the base model is 1.16207846173807"],"metadata":{}},{"cell_type":"markdown","source":["#### Linear Regression Model 2 with features as Year, State and Month. Using StringIndexer and VectorAssembler to perform One Hot Encoding"],"metadata":{}},{"cell_type":"code","source":["#Model 2 - with year, State and Month\nmodel2 = Pipeline(stages=[feature.VectorAssembler(inputCols=['Year'],\n                                        outputCol='features'),\n                          feature.StringIndexer(inputCol='Month', outputCol='encoded_Month'),\n                          feature.VectorAssembler(inputCols=['features', 'encoded_Month'], outputCol='semi_final_features'),\n                          feature.StringIndexer(inputCol='State', outputCol='encoded_State'),\n                          feature.VectorAssembler(inputCols=['semi_final_features', 'encoded_State'], outputCol='final_features'),\n                 regression.LinearRegression(featuresCol='final_features', labelCol='Illnesses_log')]).fit(training_df)"],"metadata":{},"outputs":[],"execution_count":78},{"cell_type":"code","source":["model2.transform(validation_df).select(rmse).show(100)"],"metadata":{},"outputs":[],"execution_count":79},{"cell_type":"code","source":["model2.transform(testing_df).select(rmse).show(100)"],"metadata":{"collapsed":true},"outputs":[],"execution_count":80},{"cell_type":"code","source":["model2.transform(testing_df).select((fn.col('Illnesses_log') - fn.col('prediction'))**2, fn.col('Illnesses_log'), fn.col('prediction')).show(5000)"],"metadata":{"collapsed":true},"outputs":[],"execution_count":81},{"cell_type":"markdown","source":["#### We found that the RMSE values of the model 2 is 1.13077887009722"],"metadata":{}},{"cell_type":"code","source":["#Testing String Indexer\n#indexer_model = StringIndexer(inputCol='Month', outputCol=\"{0}_indexed\".format('Month')).fit(training_df)\n#indexed_df = indexer_model.transform(training_df)\n#indexed_df.show(5)"],"metadata":{"collapsed":true},"outputs":[],"execution_count":83},{"cell_type":"markdown","source":["#### Linear Regression Model 3 with features State and Month only"],"metadata":{}},{"cell_type":"code","source":["#Model 3 \n#With state, Month only\nmodel3 = Pipeline(stages=[feature.StringIndexer(inputCol='Month', outputCol='encoded_Month'),\n                          feature.VectorAssembler(inputCols=['encoded_Month'], outputCol='semi_final_features'),\n                          feature.StringIndexer(inputCol='State', outputCol='encoded_State'),\n                          feature.VectorAssembler(inputCols=['semi_final_features', 'encoded_State'], outputCol='final_features'),\n                 regression.LinearRegression(labelCol='Illnesses_log', featuresCol='final_features')]).fit(training_df)"],"metadata":{"collapsed":true},"outputs":[],"execution_count":85},{"cell_type":"code","source":["model3.transform(validation_df).select(rmse).show()"],"metadata":{"collapsed":true},"outputs":[],"execution_count":86},{"cell_type":"code","source":["model3.transform(testing_df).select(rmse).show()"],"metadata":{"collapsed":true},"outputs":[],"execution_count":87},{"cell_type":"markdown","source":["#### We found the RMSE of model 3 is 1.1307483108756038"],"metadata":{}},{"cell_type":"markdown","source":["#### We find that Model 3 is better than Model 2, and Model 2 is better than the base model."],"metadata":{}},{"cell_type":"code","source":["#Model 3> Model 2 > Model 1"],"metadata":{"collapsed":true},"outputs":[],"execution_count":90},{"cell_type":"code","source":["#Adding String Indexer\nindexer_model = StringIndexer(inputCol='Month', outputCol=\"{0}_indexed\".format('Month')).fit(df)\nindexed_df = indexer_model.transform(df)\nindexer_model2 = StringIndexer(inputCol='State', outputCol=\"{0}_indexed\".format('State')).fit(indexed_df)\nindexed_df = indexer_model2.transform(indexed_df)\nindexed_df.toPandas()"],"metadata":{"collapsed":true},"outputs":[],"execution_count":91},{"cell_type":"markdown","source":["#### Creating a new dataframe which has states and month values indexed."],"metadata":{}},{"cell_type":"code","source":["#We have a new dataframe indexed with states and month\nindexed_df.head(5)"],"metadata":{"collapsed":true},"outputs":[],"execution_count":93},{"cell_type":"code","source":["outbreaks_new= outbreaks.copy()\noutbreaks_new.head(5)"],"metadata":{"collapsed":true},"outputs":[],"execution_count":94},{"cell_type":"markdown","source":["#### Dropping the column \"Hospitalizations\""],"metadata":{}},{"cell_type":"code","source":["outbreaks_new.drop(['Hospitalizations'], axis=1, inplace=True)\noutbreaks_new.head(1)"],"metadata":{"collapsed":true},"outputs":[],"execution_count":96},{"cell_type":"code","source":["outbreaks_new.Location.value_counts()"],"metadata":{"collapsed":true},"outputs":[],"execution_count":97},{"cell_type":"code","source":["outbreaks_new.Food.value_counts()"],"metadata":{"collapsed":true},"outputs":[],"execution_count":98},{"cell_type":"markdown","source":["#### Creating dummy variable for the column \"Location\" to pick the first location out of the list of location and similarly for food also."],"metadata":{}},{"cell_type":"code","source":["#Creating dummies for Location Variable\noutbreaks_new['Location_modified']=outbreaks_new['Location'].str.split(';').str[0]\noutbreaks_new['Food_modified']=outbreaks_new['Food'].str.split(',').str[0]\noutbreaks_new['Food_modified_new']=outbreaks_new['Food_modified'].str.split(';').str[0]"],"metadata":{"collapsed":true},"outputs":[],"execution_count":100},{"cell_type":"code","source":["outbreaks_new.Food_modified_new.value_counts()"],"metadata":{"collapsed":true},"outputs":[],"execution_count":101},{"cell_type":"code","source":["list(outbreaks_new.columns)"],"metadata":{"collapsed":true},"outputs":[],"execution_count":102},{"cell_type":"code","source":["df = spark.createDataFrame(outbreaks_new)\ndf.show(50)"],"metadata":{"collapsed":true},"outputs":[],"execution_count":103},{"cell_type":"markdown","source":["#### Performing StringIndexing and One Hot Encoding on the newly modified columns"],"metadata":{}},{"cell_type":"code","source":["categorical_columns = [\"Year\",\"Month\",\"State\", \"Location_modified\", \"Food_modified_new\"]\nstring_indexer_models = []\none_hot_encoders = []\nfor col_name in categorical_columns:\n    # OneHotEncoders map number indices column to column of binary vectors\n    string_indexer_model = StringIndexer(inputCol=col_name, outputCol=\"{0}_indexed\".format(col_name)).fit(df)\n    df = string_indexer_model.transform(df)\n    string_indexer_models.append(string_indexer_model)\n    \n    one_hot_encoder = OneHotEncoder(inputCol=\"{0}_indexed\".format(col_name), outputCol=\"{0}_encoded\".format(col_name), dropLast=False)\n    df = one_hot_encoder.transform(df)\n    \n    one_hot_encoders.append(one_hot_encoder)"],"metadata":{"collapsed":true},"outputs":[],"execution_count":105},{"cell_type":"code","source":["display(df)"],"metadata":{"collapsed":true},"outputs":[],"execution_count":106},{"cell_type":"markdown","source":["#### Displaying the correlation matrix between all the features and illnesses_log. We found that location has a very high correlation"],"metadata":{}},{"cell_type":"code","source":["#Correlation between all features and Illnesses_log\ncorr_columns = [\"Year\",\"Month_indexed\",\"State_indexed\", \"Location_modified_indexed\", \"Food_modified_new_indexed\", \"Illnesses_log\"]\ncorr_df=df.select(corr_columns).toPandas()\nplt.cla()\nsns.heatmap(corr_df.corr(),annot=True)\ndisplay()"],"metadata":{"collapsed":true},"outputs":[],"execution_count":108},{"cell_type":"code","source":["training_df, validation_df, testing_df = df.randomSplit([0.6, 0.3, 0.1])\n#display(testing_df)\ntesting_df.columns"],"metadata":{"collapsed":true},"outputs":[],"execution_count":109},{"cell_type":"code","source":["df.printSchema()"],"metadata":{"collapsed":true},"outputs":[],"execution_count":110},{"cell_type":"markdown","source":["#### Linear Regression Model 4 with features as State_Encoded and Location_modified_encoded"],"metadata":{}},{"cell_type":"code","source":["model4 = Pipeline(stages=[\n  feature.VectorAssembler(inputCols=['State_encoded', 'Location_modified_encoded'],outputCol='features'),\n  regression.LinearRegression(featuresCol='features', labelCol='Illnesses_log')]).fit(training_df)"],"metadata":{"collapsed":true},"outputs":[],"execution_count":112},{"cell_type":"code","source":["model4.transform(validation_df).select(rmse).show()"],"metadata":{"collapsed":true},"outputs":[],"execution_count":113},{"cell_type":"code","source":["model4.transform(testing_df).select(rmse).show()"],"metadata":{"collapsed":true},"outputs":[],"execution_count":114},{"cell_type":"markdown","source":["#### We found that the RMSE of model 4 is 0.9771288451129172"],"metadata":{}},{"cell_type":"markdown","source":["#### Linear Regression model 5 with features as \"State_encoded\", \"Location_modified_encoded\" and \"Food_modified_encoded\""],"metadata":{}},{"cell_type":"code","source":["model5 = Pipeline(stages=[\n  feature.VectorAssembler(inputCols=['State_encoded', 'Location_modified_encoded', 'Food_modified_new_encoded'],outputCol='features'),\n  regression.LinearRegression(featuresCol='features', labelCol='Illnesses_log',maxIter=5, regParam=0.0, elasticNetParam=0.0)]).fit(training_df)\n"],"metadata":{"collapsed":true},"outputs":[],"execution_count":117},{"cell_type":"code","source":["model5.transform(training_df).select(rmse).show()"],"metadata":{"collapsed":true},"outputs":[],"execution_count":118},{"cell_type":"code","source":["model5.transform(validation_df).select(rmse).show()"],"metadata":{"collapsed":true},"outputs":[],"execution_count":119},{"cell_type":"code","source":["model5.transform(testing_df).select(rmse).show()"],"metadata":{"collapsed":true},"outputs":[],"execution_count":120},{"cell_type":"code","source":["model5.transform(testing_df).select((fn.col('Illnesses_log') - fn.col('prediction'))**2, fn.col('Illnesses_log'), fn.col('prediction')).show(500)"],"metadata":{"collapsed":true},"outputs":[],"execution_count":121},{"cell_type":"markdown","source":["#### We found that the RMSE of the model 5 is 0.9561234226307269"],"metadata":{}},{"cell_type":"markdown","source":["#### Linear Regression model 6 with features as \", Month_encoded\",\"Year_encoded\", \"Food_encoded\", \"State_encoded and \"Location_encoded\""],"metadata":{}},{"cell_type":"code","source":["#Model 6 with all the features - Year, Month, State, Food and Location\nmodel6 = Pipeline(stages=[\n  feature.VectorAssembler(inputCols=['Month_encoded','Year_encoded','State_encoded', 'Location_modified_encoded', 'Food_modified_new_encoded'],outputCol='features'),\n  regression.LinearRegression(featuresCol='features', labelCol='Illnesses_log',maxIter=5, regParam=0.00, elasticNetParam=0.0)]).fit(training_df)\n"],"metadata":{"collapsed":true},"outputs":[],"execution_count":124},{"cell_type":"code","source":["model6.transform(training_df).select(rmse).show()"],"metadata":{"collapsed":true},"outputs":[],"execution_count":125},{"cell_type":"code","source":["model6.transform(validation_df).select(rmse).show()"],"metadata":{"collapsed":true},"outputs":[],"execution_count":126},{"cell_type":"code","source":["model6.transform(testing_df).select(rmse).show()"],"metadata":{"collapsed":true},"outputs":[],"execution_count":127},{"cell_type":"markdown","source":["#### We found that the RMSE of model 6 is 0.9522669914148608"],"metadata":{}},{"cell_type":"markdown","source":["#### Linear Regression model 7 with features as \"Month_indexed\",\"Year_indexed\",\"State_indexed\",\"Location_modified_indexed\",\"Food_modified_indexed\""],"metadata":{}},{"cell_type":"code","source":["#Model 6 modification with non encoded by indexed values\nmodel7 = Pipeline(stages=[\n  feature.VectorAssembler(inputCols=['Month_indexed','Year_indexed','State_indexed', 'Location_modified_indexed', 'Food_modified_new_indexed'],outputCol='features'),\n  regression.LinearRegression(featuresCol='features', labelCol='Illnesses_log',maxIter=5, regParam=0.0, elasticNetParam=0.0)]).fit(training_df)\n"],"metadata":{"collapsed":true},"outputs":[],"execution_count":130},{"cell_type":"code","source":["display(testing_df)\n"],"metadata":{"collapsed":true},"outputs":[],"execution_count":131},{"cell_type":"code","source":["model7.transform(training_df).select(rmse).show()"],"metadata":{"collapsed":true},"outputs":[],"execution_count":132},{"cell_type":"code","source":["model7.transform(validation_df).select(rmse).show()"],"metadata":{"collapsed":true},"outputs":[],"execution_count":133},{"cell_type":"code","source":["model7.transform(testing_df).select(rmse).show()"],"metadata":{"collapsed":true},"outputs":[],"execution_count":134},{"cell_type":"markdown","source":["#### The RMSE value of model 7 is 1.0759126056961936"],"metadata":{}},{"cell_type":"markdown","source":["#### After linear modelling, we found that taking indexes instead of encoding might not be a great idea"],"metadata":{"collapsed":true}},{"cell_type":"code","source":["import pyspark.ml.tuning as tune\ngrid = tune.ParamGridBuilder()"],"metadata":{"collapsed":true},"outputs":[],"execution_count":137},{"cell_type":"markdown","source":["#### We also found that model 6 is the best until and we will pick it for performing regularization and cross validation"],"metadata":{}},{"cell_type":"markdown","source":["#### Performing cross validation with elastic net regularization on the best Linear model - Model 6"],"metadata":{}},{"cell_type":"code","source":["reg = regression.LinearRegression(labelCol = 'Illnesses_log', featuresCol = 'features', maxIter=5)"],"metadata":{"collapsed":true},"outputs":[],"execution_count":140},{"cell_type":"code","source":["grid = grid.addGrid(reg.elasticNetParam, [0.0, 0.2, 0.4, 0.6, 0.8, 1.0])"],"metadata":{"collapsed":true},"outputs":[],"execution_count":141},{"cell_type":"code","source":["grid = grid.addGrid(reg.regParam, np.arange(0,.1,.01))"],"metadata":{"collapsed":true},"outputs":[],"execution_count":142},{"cell_type":"code","source":["np.arange(0,0.1,0.01)"],"metadata":{"collapsed":true},"outputs":[],"execution_count":143},{"cell_type":"code","source":["grid = grid.build()"],"metadata":{"collapsed":true},"outputs":[],"execution_count":144},{"cell_type":"code","source":["evaluator = RegressionEvaluator(labelCol=reg.getLabelCol(), predictionCol=reg.getPredictionCol())"],"metadata":{"collapsed":true},"outputs":[],"execution_count":145},{"cell_type":"code","source":["va= feature.VectorAssembler(inputCols=['Month_encoded','Year_encoded','State_encoded', 'Location_modified_encoded', 'Food_modified_new_encoded'],outputCol='features')"],"metadata":{"collapsed":true},"outputs":[],"execution_count":146},{"cell_type":"code","source":["crossPipe = Pipeline(stages=[va,reg])"],"metadata":{"collapsed":true},"outputs":[],"execution_count":147},{"cell_type":"code","source":["cv = tune.CrossValidator(estimator = crossPipe, estimatorParamMaps = grid, evaluator= evaluator, numFolds = 3)"],"metadata":{"collapsed":true},"outputs":[],"execution_count":148},{"cell_type":"code","source":["list1=list()"],"metadata":{"collapsed":true},"outputs":[],"execution_count":149},{"cell_type":"markdown","source":["#### Creating a method to iterate on all elastic net parameters and perform cross validation to give the best parameters"],"metadata":{}},{"cell_type":"code","source":["class CrossValidatorVerbose(CrossValidator):\n  def _fit(self, dataset):\n        est = self.getOrDefault(self.estimator)\n        epm = self.getOrDefault(self.estimatorParamMaps)\n        numModels = len(epm)\n\n        eva = self.getOrDefault(self.evaluator)\n        metricName = eva.getMetricName()\n\n        nFolds = self.getOrDefault(self.numFolds)\n        seed = self.getOrDefault(self.seed)\n        h = 1.0 / nFolds\n\n        randCol = self.uid + \"_rand\"\n        df = dataset.select(\"*\", rand(seed).alias(randCol))\n        metrics = [0.0] * numModels\n\n        for i in range(nFolds):\n            foldNum = i + 1\n            print(\"Comparing models on fold %d\" % foldNum)\n\n            validateLB = i * h\n            validateUB = (i + 1) * h\n            condition = (df[randCol] >= validateLB) & (df[randCol] < validateUB)\n            validation = df.filter(condition)\n            train = df.filter(~condition)\n\n            for j in range(numModels):\n                paramMap = epm[j]\n                model = est.fit(train, paramMap)\n                # TODO: duplicate evaluator to take extra params from input\n                metric = eva.evaluate(model.transform(validation, paramMap))\n                metrics[j] += metric\n\n                avgSoFar = metrics[j] / foldNum\n                print(\"params: %s\\t%s: %f\\tavg: %f\" % (\n                    {param.name: val for (param, val) in paramMap.items()},\n                    metricName, metric, avgSoFar))\n                list1.append([{param.name: val for (param, val) in paramMap.items()},metric,avgSoFar])\n              # paramsList.append([[{param.name: val for (param, val) in paramMap.items()},metricName,metric,avgSoFar]])\n\n        if eva.isLargerBetter():\n            bestIndex = np.argmax(metrics)\n        else:\n            bestIndex = np.argmin(metrics)\n\n        bestParams = epm[bestIndex]\n        bestModel = est.fit(dataset, bestParams)\n        avgMetrics = [m / nFolds for m in metrics]\n        bestAvg = avgMetrics[bestIndex]\n        print(\"Best model:\\nparams: %s\\t%s: %f\" % (\n            {param.name: val for (param, val) in bestParams.items()},\n            metricName, bestAvg))\n\n        return self._copyValues(CrossValidatorModel(bestModel, avgMetrics))\n      "],"metadata":{"collapsed":true},"outputs":[],"execution_count":151},{"cell_type":"code","source":["cvVer = CrossValidatorVerbose(estimator = crossPipe, estimatorParamMaps = grid, evaluator= evaluator, numFolds = 3)"],"metadata":{"collapsed":true},"outputs":[],"execution_count":152},{"cell_type":"code","source":["from pyspark.sql.functions import rand"],"metadata":{"collapsed":true},"outputs":[],"execution_count":153},{"cell_type":"markdown","source":["#### Splitting whole dataset into training and test to perform cross validation"],"metadata":{}},{"cell_type":"code","source":["training, test = df.randomSplit([0.7,0.3],0)"],"metadata":{"collapsed":true},"outputs":[],"execution_count":155},{"cell_type":"markdown","source":["#### Performing cross validation on the train and test"],"metadata":{}},{"cell_type":"code","source":["cvVer.fit(training).transform(test)"],"metadata":{"collapsed":true},"outputs":[],"execution_count":157},{"cell_type":"markdown","source":["#### Fetching 10 best parameters from the cross validation and Elastic Net Regularization"],"metadata":{}},{"cell_type":"code","source":["#Fetch Parameters automatically\nlist1\ndf_cross_val=pd.DataFrame(list1,columns=['Regularization_Parameters','RMSE','AVG'])\ndf_cross_val.head()\ndf_cross_val['Regularization_Parameters']=df_cross_val['Regularization_Parameters'].astype('str') "],"metadata":{"collapsed":true},"outputs":[],"execution_count":159},{"cell_type":"code","source":["df_cross_val['Regularization_Parameters'] = df_cross_val['Regularization_Parameters'].replace({'regParam' : 'regP'}, regex=True)\ndf_cross_val['Regularization_Parameters'] = df_cross_val['Regularization_Parameters'].replace({'elasticNetParam' : 'elNetP'}, regex=True)\ndf_cross_val['Regularization_Parameters'] = df_cross_val['Regularization_Parameters'].replace({'{' : ''}, regex=True)\ndf_cross_val['Regularization_Parameters'] = df_cross_val['Regularization_Parameters'].replace({'}' : ''}, regex=True)\n"],"metadata":{"collapsed":true},"outputs":[],"execution_count":160},{"cell_type":"code","source":["df_cross_val.head(10)"],"metadata":{"collapsed":true},"outputs":[],"execution_count":161},{"cell_type":"code","source":["df_cross_val_best=df_cross_val.nsmallest(10, 'RMSE')\ndf_cross_val_best.head()"],"metadata":{"collapsed":true},"outputs":[],"execution_count":162},{"cell_type":"markdown","source":["##### Plotting best 10 regularization with least RMSE to show a comparison"],"metadata":{}},{"cell_type":"code","source":["#Plot of regularizations with RMSE values\nplt.figure()\nfig=plt.figure(figsize=(25, 10), dpi= 60)\nsns.pointplot( y = 'RMSE', x = 'Regularization_Parameters', data = df_cross_val_best, palette='Blues',)\nplt.xticks(rotation =60)\nplt.tight_layout()\ndisplay()"],"metadata":{"collapsed":true},"outputs":[],"execution_count":164},{"cell_type":"code","source":["a = spark.createDataFrame(df_cross_val_best)\ndisplay(a)"],"metadata":{"collapsed":true},"outputs":[],"execution_count":165},{"cell_type":"markdown","source":["#### Best Value of Regular Param and Elastic Param \n#### params: {regParam': 0.029999999999999999, 'elasticNetParam': 0.2}\trmse: 0.952868, it will change everytime"],"metadata":{}},{"cell_type":"code","source":["#Best Value of Regular Param and Elastic Param \n#params: {regParam': 0.029999999999999999, 'elasticNetParam': 0.2}\trmse: 0.952868, it will change everytime\nregParamBest=0.029999999999999999\nelasticNetParamBest=0.2"],"metadata":{"collapsed":true},"outputs":[],"execution_count":167},{"cell_type":"markdown","source":["#### Putting these best values in the model 6 taken by us"],"metadata":{}},{"cell_type":"code","source":["#Putting these best values in the model 6 taken by us\nfinal_regression_model = Pipeline(stages=[\n  feature.VectorAssembler(inputCols=['Month_encoded','Year_encoded','State_encoded','Location_modified_encoded','Food_modified_new_encoded'],outputCol='features'),\n  regression.LinearRegression(featuresCol='features', labelCol='Illnesses_log',maxIter=5, regParam=regParamBest, elasticNetParam=elasticNetParamBest)]).fit(training)"],"metadata":{"collapsed":true},"outputs":[],"execution_count":169},{"cell_type":"markdown","source":["#### Utilizing the best parameter value to test the model. Observervation is that RMSE goes down further"],"metadata":{}},{"cell_type":"code","source":["final_regression_model.transform(training).select(rmse).show()\n#older value was 0.8996500501049035"],"metadata":{"collapsed":true},"outputs":[],"execution_count":171},{"cell_type":"code","source":["\ntemp_result=final_regression_model.transform(test)\ndisplay(temp_result)"],"metadata":{"collapsed":true},"outputs":[],"execution_count":172},{"cell_type":"markdown","source":["#### Plotting Scatter Plot between predicted values and actual values"],"metadata":{}},{"cell_type":"code","source":["#Plotting Linear Model\nplt.cla()\nplotting_df=temp_result.toPandas()\nplt.scatter(plotting_df['Illnesses_log'],plotting_df[\"prediction\"])\ndisplay()"],"metadata":{"collapsed":true},"outputs":[],"execution_count":174},{"cell_type":"code","source":["training_df.printSchema()"],"metadata":{"collapsed":true},"outputs":[],"execution_count":175},{"cell_type":"code","source":["finalModelFit =  cv.fit(training)"],"metadata":{"collapsed":true},"outputs":[],"execution_count":176},{"cell_type":"code","source":["final_RMSE_value = evaluator.evaluate(finalModelFit.transform(test))"],"metadata":{"collapsed":true},"outputs":[],"execution_count":177},{"cell_type":"code","source":["final_RMSE_value"],"metadata":{"collapsed":true},"outputs":[],"execution_count":178},{"cell_type":"code","source":["pred =  finalModelFit.transform(test)"],"metadata":{"collapsed":true},"outputs":[],"execution_count":179},{"cell_type":"code","source":["pred.select('Illnesses_log', 'prediction').show(500)"],"metadata":{"collapsed":true},"outputs":[],"execution_count":180},{"cell_type":"markdown","source":["#### Fetching Coefficient and R2 values for the best to interpret features importance"],"metadata":{}},{"cell_type":"code","source":["final_regression_model.stages[-1].coefficients"],"metadata":{"collapsed":true},"outputs":[],"execution_count":182},{"cell_type":"code","source":["final_regression_model.stages[-1].summary.r2"],"metadata":{"collapsed":true},"outputs":[],"execution_count":183},{"cell_type":"code","source":["coefficients_list=final_regression_model.stages[-1].coefficients\ncoefficients_list"],"metadata":{},"outputs":[],"execution_count":184},{"cell_type":"markdown","source":["####  Fetching and plotting coefficients for Year to understand their importance"],"metadata":{}},{"cell_type":"code","source":["year_list=df.toPandas()['Year'].unique()\nlen(year_list)\nyear_coefficients_list= coefficients_list[:18]\nyear_coefficients_list\nyear_coefficients = pd.DataFrame(\n    {'year': year_list,\n     'coefficients': year_coefficients_list\n    })\nyear_coefficients.head(19)"],"metadata":{"collapsed":true},"outputs":[],"execution_count":186},{"cell_type":"code","source":["#Plotting year coefficients\nplt.cla()\nfig=plt.figure(figsize=(25, 10), dpi= 80)\nsns.barplot( y = 'coefficients', x = 'year', data = year_coefficients, palette='Blues')\nplt.xticks(rotation = 60)\nplt.tight_layout()\ndisplay()"],"metadata":{"collapsed":true},"outputs":[],"execution_count":187},{"cell_type":"markdown","source":["### Year 2000, Year 2013, Year 2014 are most important factors here"],"metadata":{"collapsed":true}},{"cell_type":"markdown","source":["####  Fetching and plotting coefficients for Month to understand their importance"],"metadata":{}},{"cell_type":"code","source":["#Month coefficients\nmonth_list=df.toPandas()['Month_indexed'].unique()\nmonth_name_list=df.toPandas()['Month'].unique()\nlen(month_list)\nmonth_coefficients_list= coefficients_list[18:30]\nmonth_coefficients_list\nmonth_coefficients = pd.DataFrame(\n    {'month': month_list,\n     'coefficients': month_coefficients_list,\n     'month_name':month_name_list\n    })\nmonth_coefficients.head(13)\n"],"metadata":{"collapsed":true},"outputs":[],"execution_count":190},{"cell_type":"code","source":["#Plotting month coefficients\nplt.cla()\nfig=plt.figure(figsize=(25, 10), dpi= 80)\nsns.barplot( y = 'coefficients', x = 'month_name', data = month_coefficients, palette='GnBu_d')\nplt.xticks(rotation = 60)\nplt.tight_layout()\ndisplay()"],"metadata":{"collapsed":true},"outputs":[],"execution_count":191},{"cell_type":"markdown","source":["### Conclusion for Months\n### Jan seems to be an important one in terms of months"],"metadata":{"collapsed":true}},{"cell_type":"markdown","source":["####  Fetching and plotting coefficients for States indexed values to understand their importance"],"metadata":{}},{"cell_type":"code","source":["#For State, coffecients\nstate_list=df.toPandas()['State_indexed'].unique()\nstate_name_list=df.toPandas()['State'].unique()\nlen(state_list)\n\n"],"metadata":{"collapsed":true},"outputs":[],"execution_count":194},{"cell_type":"code","source":["state_coefficients_list= coefficients_list[30:85]\nstate_coefficients_list\nstate_coefficients = pd.DataFrame(\n    {'state': state_list,\n     'coefficients': state_coefficients_list,\n     'state_name':state_name_list\n    })\nstate_coefficients_temp=state_coefficients.nlargest(10, 'coefficients')"],"metadata":{"collapsed":true},"outputs":[],"execution_count":195},{"cell_type":"code","source":["plt.cla()\nfig=plt.figure(figsize=(25, 10), dpi= 80)\nsns.barplot( y = 'coefficients', x = 'state_name', data = state_coefficients_temp, palette='coolwarm')\nplt.xticks(rotation = 60)\nplt.tight_layout()\ndisplay()"],"metadata":{"collapsed":true},"outputs":[],"execution_count":196},{"cell_type":"markdown","source":["#### Importance of top 10 states in predicting results shown above"],"metadata":{"collapsed":true}},{"cell_type":"markdown","source":["####  Fetching and plotting coefficients for Location to understand their importance"],"metadata":{}},{"cell_type":"code","source":["#Based on Location, top most features\nlocation_list=df.toPandas()['Location_modified_indexed'].unique()\nlocation_name_list=df.toPandas()['Location_modified'].unique()\nlen(location_list)\n\n"],"metadata":{"collapsed":true},"outputs":[],"execution_count":199},{"cell_type":"code","source":["location_coefficients_list= coefficients_list[85:106]\nlocation_coefficients_list\nlocation_coefficients = pd.DataFrame(\n    {'location': location_list,\n     'coefficients': location_coefficients_list,\n     'location_name':location_name_list\n    })\nlocation_coefficients.head(13)"],"metadata":{"collapsed":true},"outputs":[],"execution_count":200},{"cell_type":"code","source":["#Plotting year coefficients\nplt.cla()\nfig=plt.figure(figsize=(25, 10), dpi= 100)\nsns.barplot( y = 'coefficients', x = 'location_name', data = location_coefficients, palette='Blues')\nplt.xticks(rotation = 90)\nplt.tight_layout()\ndisplay()"],"metadata":{"collapsed":true},"outputs":[],"execution_count":201},{"cell_type":"markdown","source":["### Important features found here are - food consumption at hospital , nursing homes, assisted living facility and school/college/University"],"metadata":{"collapsed":true}},{"cell_type":"markdown","source":["#### Fetching and plotting coefficients for Food Indexed to understand their importance"],"metadata":{}},{"cell_type":"code","source":["#Last but not least food\nfood_list=df.toPandas()['Food_modified_new_indexed'].unique()\nfood_name_list=df.toPandas()['Food_modified_new'].unique()\nlen(food_list)"],"metadata":{"collapsed":true},"outputs":[],"execution_count":204},{"cell_type":"code","source":["food_coefficients_list= coefficients_list[106:1055]\nfood_coefficients_list\nfood_coefficients = pd.DataFrame(\n    {'food': food_list,\n     'coefficients': food_coefficients_list,\n     'food_name':food_name_list\n    })\nfood_coefficients_temp=food_coefficients.nlargest(20, 'coefficients')"],"metadata":{"collapsed":true},"outputs":[],"execution_count":205},{"cell_type":"code","source":["#Plotting year coefficients\nplt.figure()\nfig=plt.figure(figsize=(25, 10), dpi= 80)\nsns.barplot( y = 'coefficients', x = 'food_name', data = food_coefficients_temp, palette='husl')\nplt.xticks(rotation = 60)\nplt.tight_layout()\ndisplay()"],"metadata":{"collapsed":true},"outputs":[],"execution_count":206},{"cell_type":"markdown","source":["### Main Food Items involved are Crab, Fries etc"],"metadata":{"collapsed":true}},{"cell_type":"code","source":["#reg_best = regression.LinearRegression(labelCol = 'Illnesses_log', featuresCol = 'features', maxIter=5, regParam=0.040000000000000001, elasticNetParam=0.2)"],"metadata":{"collapsed":true},"outputs":[],"execution_count":208},{"cell_type":"markdown","source":["#### Giving input value for features and getting predictions"],"metadata":{}},{"cell_type":"code","source":["#df_for_prof.dtypes"],"metadata":{"collapsed":true},"outputs":[],"execution_count":210},{"cell_type":"code","source":["df_for_prof = spark.createDataFrame(outbreaks_new)\ndf_for_prof.show(50)\ncategorical_columns = [\"Year\",\"Month\",\"State\", \"Location_modified\", \"Food_modified_new\"]\nstring_indexer_models = []\none_hot_encoders = []\ntraining_df_prof, validation_df_prof, testing_df_prof = df_for_prof.randomSplit([0.6, 0.3, 0.1])\n#display(testing_df)\ndisplay(testing_df_prof)\ntest_df_prof=testing_df_prof.toPandas()\ntest_df_prof.loc[-1] = [2003, \"August\", \"Utah\", \"Restaurant\", \"Lo Mein\", 4, 3, 0.111, \"Restaurant\", \"Lo Mein\", \"Lo Mein\" ]  # adding a row\ntest_df_prof.index = test_df_prof.index + 1  # shifting index\ntest_df_prof = test_df_prof.sort_index()"],"metadata":{"collapsed":true},"outputs":[],"execution_count":211},{"cell_type":"code","source":["testing_df_prof=spark.createDataFrame(test_df_prof)\ndisplay(testing_df_prof)"],"metadata":{"collapsed":true},"outputs":[],"execution_count":212},{"cell_type":"code","source":["for col_name in categorical_columns:\n    # OneHotEncoders map number indices column to column of binary vectors\n    string_indexer_model = StringIndexer(inputCol=col_name, outputCol=\"{0}_indexed\".format(col_name)).fit(testing_df_prof)\n    testing_df_prof = string_indexer_model.transform(testing_df_prof)\n    string_indexer_models.append(string_indexer_model)\n    \n    one_hot_encoder = OneHotEncoder(inputCol=\"{0}_indexed\".format(col_name), outputCol=\"{0}_encoded\".format(col_name), dropLast=False)\n    testing_df_prof = one_hot_encoder.transform(testing_df_prof)\n    \n    one_hot_encoders.append(one_hot_encoder)\ndisplay(testing_df_prof)"],"metadata":{"collapsed":true},"outputs":[],"execution_count":213},{"cell_type":"code","source":["#model6 = Pipeline(stages=[\n#  feature.VectorAssembler(inputCols=['Month_encoded','Year_encoded','State_encoded', 'Location_modified_encoded', 'Food_modified_new_encoded'],outputCol='features'),\n#  regression.LinearRegression(featuresCol='features', labelCol='Illnesses_log',maxIter=5, regParam=0.00, elasticNetParam=0.0)]).fit(training_df)#\n#model6.transform(testing_df_prof).select(fn.col('prediction')).show(5)\n#training_df.dtypes"],"metadata":{"collapsed":true},"outputs":[],"execution_count":214},{"cell_type":"markdown","source":["#### Adding few more models to check the RMSE , first one is Generalized Linear Regression - Model 8"],"metadata":{"collapsed":true}},{"cell_type":"code","source":["model8 = Pipeline(stages=[\n  va,\n  regression.GeneralizedLinearRegression(family=\"gaussian\", link=\"identity\",featuresCol='features', labelCol='Illnesses_log',  maxIter=5, regParam=0.0 )]).fit(training_df)"],"metadata":{"collapsed":true},"outputs":[],"execution_count":216},{"cell_type":"code","source":["model8.transform(validation_df).select(rmse).show()"],"metadata":{"collapsed":true},"outputs":[],"execution_count":217},{"cell_type":"code","source":["model8.transform(testing_df).select(rmse).show()"],"metadata":{"collapsed":true},"outputs":[],"execution_count":218},{"cell_type":"code","source":["training_df.printSchema()"],"metadata":{"collapsed":true},"outputs":[],"execution_count":219},{"cell_type":"code","source":["display(df)"],"metadata":{"collapsed":true},"outputs":[],"execution_count":220},{"cell_type":"markdown","source":["#### Using Decision Tree Regression - Model 9"],"metadata":{}},{"cell_type":"code","source":["#Decision Tree Regression\nmodel9 = Pipeline(stages=[\nva,\nregression.DecisionTreeRegressor(featuresCol='features', labelCol='Illnesses_log')]).fit(training_df)"],"metadata":{"collapsed":true},"outputs":[],"execution_count":222},{"cell_type":"code","source":["model9.transform(validation_df).select(rmse).show()"],"metadata":{"collapsed":true},"outputs":[],"execution_count":223},{"cell_type":"code","source":["model9.transform(testing_df).select(rmse).show()"],"metadata":{"collapsed":true},"outputs":[],"execution_count":224},{"cell_type":"markdown","source":["#### Using Random Forest Regression - Model 10"],"metadata":{}},{"cell_type":"code","source":["#Random Forest regression\nmodel10 = Pipeline(stages=[\nva,\nregression.RandomForestRegressor(featuresCol='features', labelCol='Illnesses_log')]).fit(training_df)"],"metadata":{"collapsed":true},"outputs":[],"execution_count":226},{"cell_type":"code","source":["model10.transform(validation_df).select(rmse).show()"],"metadata":{"collapsed":true},"outputs":[],"execution_count":227},{"cell_type":"code","source":["model10.transform(testing_df).select(rmse).show()"],"metadata":{"collapsed":true},"outputs":[],"execution_count":228},{"cell_type":"markdown","source":["#### Using Gradient Boosting Regression - Model 11"],"metadata":{}},{"cell_type":"code","source":["#Gradient Boosting Regression\nmodel11 = Pipeline(stages=[\nva,\nregression.GBTRegressor(featuresCol='features', labelCol='Illnesses_log')]).fit(training_df)"],"metadata":{"collapsed":true},"outputs":[],"execution_count":230},{"cell_type":"code","source":["model11.transform(validation_df).select(rmse).show()"],"metadata":{"collapsed":true},"outputs":[],"execution_count":231},{"cell_type":"code","source":["model11.transform(testing_df).select(rmse).show()"],"metadata":{"collapsed":true},"outputs":[],"execution_count":232},{"cell_type":"markdown","source":["### The best Model has been found with linear regression with set elastic and normal regularization parameters - Model 6"],"metadata":{}},{"cell_type":"code","source":["#The best Model has been found with linear regression with set elastic and normal regularization parameters - Model 6"],"metadata":{"collapsed":true},"outputs":[],"execution_count":234},{"cell_type":"markdown","source":["## Time Series Analysis based on Year and Month to understand the trends of illnesses over the years"],"metadata":{}},{"cell_type":"markdown","source":["#### Bringing data into proper shape before performing Time Series Analysis"],"metadata":{}},{"cell_type":"code","source":["#Trying Time Series Analysis\noutbreaks_time_series = outbreaks_new.copy()"],"metadata":{"collapsed":true},"outputs":[],"execution_count":237},{"cell_type":"code","source":["outbreaks_time_series.head()"],"metadata":{"collapsed":true},"outputs":[],"execution_count":238},{"cell_type":"code","source":["import calendar\nd = {'January':'01', 'February':'02', 'March':'03', 'April':'04','May':'05', 'June':'06', 'July':'07', 'August':'08', 'Spetember':'09','October':'10', 'November':'11', 'December':'12' }"],"metadata":{"collapsed":true},"outputs":[],"execution_count":239},{"cell_type":"code","source":["outbreaks_time_series.Month = outbreaks_time_series.Month.map(d)"],"metadata":{"collapsed":true},"outputs":[],"execution_count":240},{"cell_type":"code","source":["outbreaks_time_series.head(5)\noutbreaks_time_series.Month.value_counts()"],"metadata":{"collapsed":true},"outputs":[],"execution_count":241},{"cell_type":"code","source":["outbreaks_time_series.Year= outbreaks_time_series[\"Year\"].map(str)+ \"-\" + outbreaks_time_series[\"Month\"]"],"metadata":{"collapsed":true},"outputs":[],"execution_count":242},{"cell_type":"code","source":["outbreaks_time_series.dtypes"],"metadata":{"collapsed":true},"outputs":[],"execution_count":243},{"cell_type":"code","source":["outbreaks_time_series.Year= outbreaks_time_series[\"Year\"]+ \"-\" + \"01\"\n"],"metadata":{"collapsed":true},"outputs":[],"execution_count":244},{"cell_type":"code","source":["outbreaks_time_series.Year=outbreaks_time_series['Year']"],"metadata":{"collapsed":true},"outputs":[],"execution_count":245},{"cell_type":"code","source":["outbreaks_time_series.head()"],"metadata":{"collapsed":true},"outputs":[],"execution_count":246},{"cell_type":"code","source":["outbreaks_time_series['Year']=pd.to_datetime(outbreaks_time_series.Year, format=\"%Y-%m-%d\")"],"metadata":{"collapsed":true},"outputs":[],"execution_count":247},{"cell_type":"code","source":["time_series_model_df=pd.DataFrame(outbreaks_time_series.groupby('Year')['Illnesses'].sum()).copy()"],"metadata":{"collapsed":true},"outputs":[],"execution_count":248},{"cell_type":"code","source":["time_series_model_df.index"],"metadata":{"collapsed":true},"outputs":[],"execution_count":249},{"cell_type":"markdown","source":["#### Forming a line plot to understand the trend of time on the Illnesses"],"metadata":{}},{"cell_type":"code","source":["plt.cla()\ntime_series_model_df.plot(figsize=(20,10), linewidth=5, fontsize=20)\nplt.xlabel('Year', fontsize=20)\ndisplay()"],"metadata":{"collapsed":true},"outputs":[],"execution_count":251},{"cell_type":"markdown","source":["#### Removing the noise from previous time series plot by forming a rolling average plot of time vs Illnesses"],"metadata":{}},{"cell_type":"code","source":["#Rolling Average\nplt.cla()\nillnesses = time_series_model_df[['Illnesses']]\nillnesses.rolling(12).mean().plot(figsize=(20,10), linewidth=5, fontsize=20)\nplt.xlabel('Year', fontsize=20)\ndisplay()"],"metadata":{"collapsed":true},"outputs":[],"execution_count":253},{"cell_type":"markdown","source":["#### The rolling average plot shows a clear decreasing trend in terms of illnesses over years"],"metadata":{}},{"cell_type":"code","source":["time_series_analysis_df=illnesses.rolling(12).mean().reset_index()\ntime_series_analysis_df = time_series_analysis_df[np.isfinite(time_series_analysis_df['Illnesses'])]\ntime_series_analysis_df.reset_index(inplace=True)\ntime_series_analysis_df.drop(['index'], axis=1,inplace=True)\ntime_series_analysis_df.reset_index(inplace=True)\ntime_series_analysis_df"],"metadata":{"collapsed":true},"outputs":[],"execution_count":255},{"cell_type":"markdown","source":["#### Understanding the average number of illnesses for every month since 1999"],"metadata":{}},{"cell_type":"code","source":["time_series_analysis_df.count()"],"metadata":{"collapsed":true},"outputs":[],"execution_count":257},{"cell_type":"markdown","source":["### Cleaning data to create a regression plot between Date and Illnesses"],"metadata":{}},{"cell_type":"code","source":["df_time_series = spark.createDataFrame(time_series_analysis_df)\ndisplay(df_time_series)\ndf_time_series.describe\n"],"metadata":{"collapsed":true},"outputs":[],"execution_count":259},{"cell_type":"code","source":["import pyspark.sql.functions as fn\nfrom pyspark.sql.types import *\ndf_time_series= df_time_series.select(fn.unix_timestamp(fn.col('Year'), format='yyyy-MM-dd HH:mm:ss.000').alias('date'),'index', 'Illnesses')\n"],"metadata":{"collapsed":true},"outputs":[],"execution_count":260},{"cell_type":"code","source":["df_time_series.show()"],"metadata":{"collapsed":true},"outputs":[],"execution_count":261},{"cell_type":"code","source":["training = df_time_series.where((df_time_series['index'] >= 0) & (df_time_series['index']<150))\ndisplay(training)"],"metadata":{"collapsed":true},"outputs":[],"execution_count":262},{"cell_type":"code","source":["testing = df_time_series.where((df_time_series['index'] >= 150) & (df_time_series['index']<=187))\ndisplay(testing)"],"metadata":{"collapsed":true},"outputs":[],"execution_count":263},{"cell_type":"code","source":["#training, testing = df_time_series.randomSplit([0.8, 0.2#], 0.0)"],"metadata":{"collapsed":true},"outputs":[],"execution_count":264},{"cell_type":"markdown","source":["### Creating a linear model based on date (timestamp) to check possible linearity in illnesses with time"],"metadata":{}},{"cell_type":"code","source":["model_time = Pipeline(stages=[\n  feature.VectorAssembler(inputCols=['date'], outputCol='features'),\n  regression.LinearRegression(featuresCol='features', labelCol='Illnesses',maxIter=5, regParam=0.01, elasticNetParam=0.2)  \n]).fit(training)"],"metadata":{"collapsed":true},"outputs":[],"execution_count":266},{"cell_type":"code","source":["rmse_time = fn.sqrt(fn.avg((fn.col('Illnesses') - fn.col('prediction'))**2))\ntest_model_time=model_time.transform(testing).select(((fn.col('Illnesses') - fn.col('prediction'))**2),fn.col('Illnesses'), fn.col('prediction'), fn.col('date'))"],"metadata":{"collapsed":true},"outputs":[],"execution_count":267},{"cell_type":"code","source":["test_model_time.show(30)"],"metadata":{"collapsed":true},"outputs":[],"execution_count":268},{"cell_type":"code","source":["model_time.transform(testing).select(rmse_time).show()"],"metadata":{"collapsed":true},"outputs":[],"execution_count":269},{"cell_type":"markdown","source":["#### Plotting a scatter plot for time vs Illnesses"],"metadata":{}},{"cell_type":"code","source":["plt.cla()\na4_dims = (11.7, 8.27)\nfig, ax = plt.subplots(figsize=a4_dims)\ng=sns.FacetGrid(data=df_time_series.toPandas(),size=8) #mapping maps in the grids using facetgrid\ng.map(plt.scatter, 'date', 'Illnesses')\ndisplay()"],"metadata":{"collapsed":true},"outputs":[],"execution_count":271},{"cell_type":"code","source":["test_model_df=test_model_time.toPandas()"],"metadata":{"collapsed":true},"outputs":[],"execution_count":272},{"cell_type":"code","source":["test_model_df.count()"],"metadata":{"collapsed":true},"outputs":[],"execution_count":273},{"cell_type":"markdown","source":["#### Fitting Linear model with trend of data to create an amazing visualization"],"metadata":{}},{"cell_type":"code","source":["plt.cla()\nplt.plot(test_model_df.date, test_model_df.Illnesses, color='g')\nplt.plot(test_model_df.date, test_model_df.prediction, color='orange')\nplt.xlabel('Years 1998-2015')\nplt.ylabel('Illnesses Occurred')\nplt.title('Illnesses vs Years - Linear Regression Basic')\nplt.show()\ndisplay()"],"metadata":{"collapsed":true},"outputs":[],"execution_count":275},{"cell_type":"markdown","source":["## Performing Logistic Regression to classify Illnesses as High Scaled or Low Scaled"],"metadata":{}},{"cell_type":"markdown","source":["#### Analyzing the data to understand the threshold value on which we will be dividing the illnesses as high or low"],"metadata":{}},{"cell_type":"code","source":["#Logistic Modeling\nmodel_time.transform(testing).select(((fn.col('Illnesses') - fn.col('prediction'))**2)).show(200)"],"metadata":{"collapsed":true},"outputs":[],"execution_count":278},{"cell_type":"code","source":["display(df)"],"metadata":{"collapsed":true},"outputs":[],"execution_count":279},{"cell_type":"code","source":["df.select(fn.avg(\"Illnesses_log\")).show()"],"metadata":{"collapsed":true},"outputs":[],"execution_count":280},{"cell_type":"code","source":["df.select(fn.max(\"Illnesses_log\")).show()"],"metadata":{"collapsed":true},"outputs":[],"execution_count":281},{"cell_type":"code","source":["df.select(fn.min(\"Illnesses_log\")).show()"],"metadata":{"collapsed":true},"outputs":[],"execution_count":282},{"cell_type":"code","source":["outbreaks_pandas_df=df.toPandas()"],"metadata":{"collapsed":true},"outputs":[],"execution_count":283},{"cell_type":"code","source":["outbreaks_pandas_df.head()"],"metadata":{"collapsed":true},"outputs":[],"execution_count":284},{"cell_type":"markdown","source":["#### Checking Distribution of Illnesses_log (logarithmic scale of Illnesses)"],"metadata":{}},{"cell_type":"code","source":["#Check Distribution of Illnesses_log\nplt.cla()\nsns.distplot(outbreaks_pandas_df['Illnesses_log'], kde=False, bins=30)\ndisplay()"],"metadata":{"collapsed":true},"outputs":[],"execution_count":286},{"cell_type":"markdown","source":["#### Jointplot between Illnesses count and Illnesses_log"],"metadata":{}},{"cell_type":"code","source":["#jointplot between Illnesses count and Illnesses_log\nplt.cla()\nsns.jointplot(x='Illnesses', y='Illnesses_log', data=outbreaks_pandas_df , kind='kde')\ndisplay()"],"metadata":{"collapsed":true},"outputs":[],"execution_count":288},{"cell_type":"markdown","source":["#### From the Viz. , we got to know that data is uniformly distributed across Illness_log value 2 (on scale of 0 - 7.5)"],"metadata":{}},{"cell_type":"code","source":["print outbreaks_pandas_df[outbreaks_pandas_df.Illnesses_log >=3]['Illnesses']"],"metadata":{"collapsed":true},"outputs":[],"execution_count":290},{"cell_type":"code","source":["print outbreaks_pandas_df[outbreaks_pandas_df.Illnesses_log >=2].count()"],"metadata":{"collapsed":true},"outputs":[],"execution_count":291},{"cell_type":"code","source":["outbreaks_pandas_df['Illnesses_impact'] = np.where(outbreaks_pandas_df['Illnesses_log']>=2, 1, 0)"],"metadata":{"collapsed":true},"outputs":[],"execution_count":292},{"cell_type":"code","source":["outbreaks_pandas_df.head()"],"metadata":{"collapsed":true},"outputs":[],"execution_count":293},{"cell_type":"markdown","source":["#### Importing package pipe from Professor Acuna's github repository"],"metadata":{}},{"cell_type":"code","source":["from pyspark_pipes import pipe"],"metadata":{"collapsed":true},"outputs":[],"execution_count":295},{"cell_type":"code","source":["df_log = spark.createDataFrame(outbreaks_pandas_df)\ndisplay(df_log)"],"metadata":{"collapsed":true},"outputs":[],"execution_count":296},{"cell_type":"markdown","source":["#### Using randomsplit to divide the df_log dataframe into training and testing datasets"],"metadata":{}},{"cell_type":"code","source":["training_df2,testing_df2 = df_log.randomSplit([0.8, 0.2], 0)\ndisplay(training_df2)"],"metadata":{"collapsed":true},"outputs":[],"execution_count":298},{"cell_type":"markdown","source":["#### Logistic Regression model with features as \"Month_encoded\", \"Year_Encoded, \"State_Encoded\", \"Location_modified_encoded\" and \"Food_modified_new_encoded\""],"metadata":{}},{"cell_type":"code","source":["model_class1 = pipe(feature.VectorAssembler(inputCols=['Month_encoded','Year_encoded','State_encoded', 'Location_modified_encoded', 'Food_modified_new_encoded'],outputCol='features'),\n     classification.LogisticRegression(labelCol='Illnesses_impact'))"],"metadata":{"collapsed":true},"outputs":[],"execution_count":300},{"cell_type":"code","source":["model_class1_fitted = model_class1.fit(training_df2)"],"metadata":{"collapsed":true},"outputs":[],"execution_count":301},{"cell_type":"code","source":["model_class1_fitted.transform(testing_df2)"],"metadata":{"collapsed":true},"outputs":[],"execution_count":302},{"cell_type":"markdown","source":["#### Defining a method to perform binary classifier evaluation"],"metadata":{}},{"cell_type":"code","source":["def binary_evaluation(model_pipeline, model_fitted, data):\n  return BinaryClassificationEvaluator(labelCol=model_pipeline.getStages()[-1].getLabelCol(), \n                                rawPredictionCol=model_pipeline.getStages()[-1].getRawPredictionCol()).\\\n    evaluate(model_fitted.transform(data))"],"metadata":{"collapsed":true},"outputs":[],"execution_count":304},{"cell_type":"code","source":["model1ROC_test=binary_evaluation(model_class1,model_class1_fitted,testing_df2)\nmodel1ROC_test\n#base accuracy 0.7563"],"metadata":{"collapsed":true},"outputs":[],"execution_count":305},{"cell_type":"code","source":["model1ROC_train=binary_evaluation(model_class1,model_class1_fitted,training_df2)\nmodel1ROC_train"],"metadata":{"collapsed":true},"outputs":[],"execution_count":306},{"cell_type":"markdown","source":["#### Using feature scaling to analyse if the ROC is increasing"],"metadata":{}},{"cell_type":"code","source":["model_class2 = pipe(feature.VectorAssembler(inputCols=['Month_encoded','Year_encoded','State_encoded', 'Location_modified_encoded', 'Food_modified_new_encoded'],outputCol='features'),\n                    feature.StandardScaler(withMean=True),\n     classification.LogisticRegression(labelCol='Illnesses_impact'))\n"],"metadata":{"collapsed":true},"outputs":[],"execution_count":308},{"cell_type":"code","source":["model_class2_fitted = model_class2.fit(training_df2)"],"metadata":{"collapsed":true},"outputs":[],"execution_count":309},{"cell_type":"code","source":["model_class2_fitted.transform(testing_df2)"],"metadata":{"collapsed":true},"outputs":[],"execution_count":310},{"cell_type":"code","source":["model2ROC_train=binary_evaluation(model_class2,model_class2_fitted,training_df2)\nmodel2ROC_train"],"metadata":{"collapsed":true},"outputs":[],"execution_count":311},{"cell_type":"markdown","source":["#### We found that feature scaling didn't help and provide results that we wanted"],"metadata":{}},{"cell_type":"code","source":["\nmodel2ROC_test=binary_evaluation(model_class2,model_class2_fitted,testing_df2)\nmodel2ROC_test"],"metadata":{"collapsed":true},"outputs":[],"execution_count":313},{"cell_type":"markdown","source":["### Adding Regularization and Cross Validation"],"metadata":{}},{"cell_type":"code","source":["lr = classification.LogisticRegression(labelCol='Illnesses_impact', featuresCol = 'features', maxIter=5)\nlr.getPredictionCol()"],"metadata":{"collapsed":true},"outputs":[],"execution_count":315},{"cell_type":"code","source":["\nparamGrid = ParamGridBuilder() \\\n    .addGrid(lr.elasticNetParam, [0., 0.2, 0.4, 0.6, 0.8, 1.0]) \\\n    .addGrid(lr.regParam, [ 0. ,  0.01,  0.02,  0.03,  0.04,  0.05,  0.06,  0.07,  0.08,  0.09]) \\\n    .build()"],"metadata":{"collapsed":true},"outputs":[],"execution_count":316},{"cell_type":"code","source":["evaluator2 = BinaryClassificationEvaluator(labelCol=lr.getLabelCol(), rawPredictionCol=lr.getPredictionCol())\ncrossPipe2 = Pipeline(stages=[va,lr])"],"metadata":{"collapsed":true},"outputs":[],"execution_count":317},{"cell_type":"code","source":["cv2 = tune.CrossValidator(estimator = crossPipe2, estimatorParamMaps = paramGrid, evaluator= evaluator2, numFolds = 2)"],"metadata":{"collapsed":true},"outputs":[],"execution_count":318},{"cell_type":"markdown","source":["#### Getting the best model from cross validation and fitting it"],"metadata":{}},{"cell_type":"code","source":["final_class_model_fitted = cv2.fit(training_df2)"],"metadata":{"collapsed":true},"outputs":[],"execution_count":320},{"cell_type":"markdown","source":["#### Carrying out are under ROC of the best fitted model"],"metadata":{}},{"cell_type":"code","source":["model3ROC_test=evaluator2.evaluate(final_class_model_fitted.transform(testing_df2))\nmodel3ROC_test"],"metadata":{"collapsed":true},"outputs":[],"execution_count":322},{"cell_type":"code","source":["model3ROC_train=evaluator2.evaluate(final_class_model_fitted.transform(training_df2))\nmodel3ROC_train"],"metadata":{"collapsed":true},"outputs":[],"execution_count":323},{"cell_type":"code","source":["#Attempt\ntraining_df3, validation_df3, testing_df3 = df_log.randomSplit([0.6, 0.3, 0.1])\ndisplay(training_df3)\nfeature_assembler = VectorAssembler(inputCols=['Month_encoded','Year_encoded','State_encoded', 'Location_modified_encoded', 'Food_modified_new_encoded'], outputCol=\"features\")\nassembled_train_df = feature_assembler.transform(training_df3).cache()\n\n"],"metadata":{"collapsed":true},"outputs":[],"execution_count":324},{"cell_type":"code","source":["assembled_validation_df = feature_assembler.transform(validation_df3).cache()"],"metadata":{"collapsed":true},"outputs":[],"execution_count":325},{"cell_type":"code","source":["assembled_test_df = feature_assembler.transform(testing_df3).cache()"],"metadata":{"collapsed":true},"outputs":[],"execution_count":326},{"cell_type":"code","source":["assembled_train_df.columns"],"metadata":{"collapsed":true},"outputs":[],"execution_count":327},{"cell_type":"markdown","source":["#### Analysing how we specify the class_weight using the weightCol feature"],"metadata":{}},{"cell_type":"code","source":["log_reg = LogisticRegression(featuresCol='features', labelCol='Illnesses_impact', maxIter=20, family='binomial')\nevaluator = BinaryClassificationEvaluator(rawPredictionCol='rawPrediction', labelCol='Illnesses_impact', metricName='areaUnderROC')"],"metadata":{"collapsed":true},"outputs":[],"execution_count":329},{"cell_type":"code","source":["model = log_reg.fit(assembled_train_df)"],"metadata":{},"outputs":[],"execution_count":330},{"cell_type":"code","source":["train_preds = model.transform(assembled_validation_df)"],"metadata":{"collapsed":true},"outputs":[],"execution_count":331},{"cell_type":"code","source":["print(train_preds.columns)"],"metadata":{"collapsed":true},"outputs":[],"execution_count":332},{"cell_type":"code","source":["train_areaUnderROC = evaluator.evaluate(train_preds)\ntrain_areaUnderROC"],"metadata":{"collapsed":true},"outputs":[],"execution_count":333},{"cell_type":"code","source":["trainpredlbls = train_preds.select(\"prediction\", \"Illnesses_impact\").cache()"],"metadata":{"collapsed":true},"outputs":[],"execution_count":334},{"cell_type":"code","source":["trainpredlbls.limit(500).toPandas()"],"metadata":{"collapsed":true},"outputs":[],"execution_count":335},{"cell_type":"markdown","source":["#### Creating a method to calculate the accuracy"],"metadata":{}},{"cell_type":"code","source":["def accuracy(predlbls):\n    counttotal = predlbls.count()\n    correct = predlbls.filter(col('Illnesses_impact') == col(\"prediction\")).count()\n    wrong = predlbls.filter(col('Illnesses_impact') != col(\"prediction\")).count()\n    ratioCorrect = float(correct)/counttotal\n    print(\"Correct: {0}, Wrong: {1}, Model Accuracy: {2}\".format(correct, wrong, np.round(ratioCorrect, 2)))"],"metadata":{"collapsed":true},"outputs":[],"execution_count":337},{"cell_type":"code","source":["accuracy(trainpredlbls)"],"metadata":{"collapsed":true},"outputs":[],"execution_count":338},{"cell_type":"code","source":["train_summary = model.evaluate(assembled_train_df)\nvalidation_summary = model.evaluate(assembled_validation_df)"],"metadata":{"collapsed":true},"outputs":[],"execution_count":339},{"cell_type":"code","source":["print('Training Accuracy   :', train_summary.accuracy)\nprint('Validation Accuracy :', validation_summary.accuracy)"],"metadata":{"collapsed":true},"outputs":[],"execution_count":340},{"cell_type":"code","source":["train_summary.areaUnderROC\n"],"metadata":{"collapsed":true},"outputs":[],"execution_count":341},{"cell_type":"markdown","source":["#### Calculating the summary of area under ROC"],"metadata":{}},{"cell_type":"code","source":["validation_summary.areaUnderROC"],"metadata":{"collapsed":true},"outputs":[],"execution_count":343},{"cell_type":"code","source":["validation_summary.fMeasureByLabel(beta=1.0)"],"metadata":{"collapsed":true},"outputs":[],"execution_count":344},{"cell_type":"code","source":["validation_summary.precisionByLabel"],"metadata":{"collapsed":true},"outputs":[],"execution_count":345},{"cell_type":"markdown","source":["#### Our model should at least perform better than the Null Accuracy. Null Accuracy is defined as the accuracy we would have got if we would have blindly predicted the majority class of the training set as the label"],"metadata":{"collapsed":true}},{"cell_type":"markdown","source":["#### Looking for Base model, Null accuracy"],"metadata":{}},{"cell_type":"code","source":["train_total = trainpredlbls.count()\ntrain_label0count = float(trainpredlbls.filter(col(\"Illnesses_impact\") == 0.0).count())\ntrain_label1count = float(trainpredlbls.filter(col(\"Illnesses_impact\") == 1.0).count())"],"metadata":{"collapsed":true},"outputs":[],"execution_count":348},{"cell_type":"markdown","source":["#### If we had predicted everything to be the majority lable, then what would be the accuracy"],"metadata":{}},{"cell_type":"code","source":["max(train_label0count, train_label1count) / train_total\n"],"metadata":{"collapsed":true},"outputs":[],"execution_count":350},{"cell_type":"markdown","source":["#### Test Accuracy"],"metadata":{}},{"cell_type":"code","source":["test_preds = model.transform(assembled_test_df)"],"metadata":{"collapsed":true},"outputs":[],"execution_count":352},{"cell_type":"code","source":["test_areaUnderROC = evaluator.evaluate(test_preds)\ntest_areaUnderROC"],"metadata":{"collapsed":true},"outputs":[],"execution_count":353},{"cell_type":"code","source":["testpredlbls = test_preds.select(\"prediction\", \"Illnesses_impact\")"],"metadata":{"collapsed":true},"outputs":[],"execution_count":354},{"cell_type":"code","source":["accuracy(testpredlbls)"],"metadata":{"collapsed":true},"outputs":[],"execution_count":355},{"cell_type":"code","source":["test_summary = model.evaluate(assembled_test_df)"],"metadata":{"collapsed":true},"outputs":[],"execution_count":356},{"cell_type":"code","source":["test_summary.accuracy"],"metadata":{"collapsed":true},"outputs":[],"execution_count":357},{"cell_type":"code","source":["test_summary.areaUnderROC"],"metadata":{"collapsed":true},"outputs":[],"execution_count":358},{"cell_type":"code","source":["test_summary.fMeasureByLabel(beta=1.0)"],"metadata":{"collapsed":true},"outputs":[],"execution_count":359},{"cell_type":"code","source":["test_summary.precisionByLabel"],"metadata":{"collapsed":true},"outputs":[],"execution_count":360},{"cell_type":"code","source":["test_summary.recallByLabel"],"metadata":{"collapsed":true},"outputs":[],"execution_count":361},{"cell_type":"code","source":["test_summary.roc.limit(10).toPandas()"],"metadata":{"collapsed":true},"outputs":[],"execution_count":362},{"cell_type":"code","source":["train_roc_pdf = train_summary.roc.toPandas()\nvalidation_roc_pdf = validation_summary.roc.toPandas()\ntest_roc_pdf = test_summary.roc.toPandas()"],"metadata":{"collapsed":true},"outputs":[],"execution_count":363},{"cell_type":"markdown","source":["#### Plotting the ROC curve for the best logistic regression model with regularization and corss validation implemented. The features are the same as the best linear model."],"metadata":{}},{"cell_type":"code","source":["plt.figure(figsize=(6,4))\nplt.plot(train_roc_pdf['FPR'], train_roc_pdf['TPR'], lw=1, label='Train AUC = %0.2f' % (train_summary.areaUnderROC))\nplt.plot(validation_roc_pdf['FPR'], validation_roc_pdf['TPR'], lw=1, label='Validation AUC = %0.2f' % (test_summary.areaUnderROC))\nplt.plot(test_roc_pdf['FPR'], test_roc_pdf['TPR'], lw=1, label='Test AUC = %0.2f' % (validation_summary.areaUnderROC))\nplt.plot([0, 1], [0, 1], '--', color=(0.6, 0.6, 0.6), label='NULL Accuracy')\nplt.title('ROC AUC Curve')\nplt.tight_layout()\nplt.legend(loc=\"best\" )\ndisplay()"],"metadata":{"collapsed":true},"outputs":[],"execution_count":365},{"cell_type":"markdown","source":["## Conclusion : \n## Use our platform to input year, month, state, Location and Food being consumed and in return our platform will forecast the amount of illnesses that can be produced based on inputs with an RMSE of 0.95. The platform also categorized illness as High or Low level with an AUC of 0.77. Lastly, the platform has a future scope of Time Series Forecasting to detect the illnesses trends in advance.  Stay Alert stay Healthy!"],"metadata":{}}],"metadata":{"kernelspec":{"display_name":"Python 2","language":"python","name":"python2"},"language_info":{"mimetype":"text/x-python","name":"python","pygments_lexer":"ipython2","codemirror_mode":{"name":"ipython","version":2},"version":"2.7.14","nbconvert_exporter":"python","file_extension":".py"},"name":"Project_Work","notebookId":3928965393511293},"nbformat":4,"nbformat_minor":0}
