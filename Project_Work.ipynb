{"cells":[{"cell_type":"code","source":["import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom pyspark.sql.types import *\nimport pyspark.sql.functions as F\nfrom pyspark.sql.functions import col, udf\n\nfrom pyspark.ml.stat import Correlation\n\nfrom pyspark.ml.classification import LogisticRegression\n\nfrom pyspark.ml.tuning import ParamGridBuilder, CrossValidator, CrossValidatorModel\n\nfrom pyspark.ml.feature import Bucketizer, StringIndexer, OneHotEncoder, StandardScaler, VectorAssembler\n\nfrom pyspark.ml import Pipeline, PipelineModel\nfrom pyspark.ml.tuning import ParamGridBuilder, CrossValidator, CrossValidatorModel\n\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\nfrom pyspark.mllib.evaluation import MulticlassMetrics\nfrom pyspark.ml.evaluation import RegressionEvaluator\n%matplotlib inline\n"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["outbreaks = pd.read_csv(\"/dbfs/FileStore/tables/outbreaks.csv\")\noutbreaks.head(10)"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["#check stats\n#Null values for ingredients and serotype/genotype has been high\noutbreaks.isnull().sum()"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["#Overall records in the dataset are 19119 which are very almost close to null values in Ingredient and Serotype,\n#so, we shouldn't consider these values as it might mislead us, its mentioned below too"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["plt.cla()\nsns.heatmap(outbreaks.isnull(), yticklabels=False, cbar=False, cmap='viridis')\nplt.tight_layout()\ndisplay()"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["outbreaks =outbreaks.rename(index=str, columns={\"Serotype/Genotype\": \"Serotype\"})"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["plt.subplots(figsize=(20,15))\nsns.countplot(x='Species', data=outbreaks, order= outbreaks.Species.value_counts().iloc[2:8].index)\ndisplay()"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["plt.cla()\ndf2 = pd.pivot_table(outbreaks, index='State', values='Illnesses', aggfunc='count')\nax = df2.plot(kind='bar', color='steelblue',figsize=(25,10))\nplt.title('Foodborne Illnesses Cases By Year')\nplt.ylabel('Illiness Cases')\ndisplay()"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["outbreaks.Food.value_counts()"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["outbreaks.drop(['Ingredient', 'Serotype', 'Species', 'Status', 'Fatalities'], axis=1, inplace=True)\noutbreaks.head()"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["plt.cla()\nsns.heatmap(outbreaks.isnull(), yticklabels=False, cbar=False, cmap='viridis')\ndisplay()"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["#Checking Null values in location\noutbreaks.isnull().sum()"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["#Checking count of values in dataset\noutbreaks.count()"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["#How illnesses are distributed over dataset\nplt.cla()\nsns.distplot(outbreaks.Illnesses, bins=10, color='red')\nplt.title('Distribution of Illnesses in Traning Set')\ndisplay()"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["#Distribution of FoodBorne Illnesses by State\nplt.cla()\ndf2 = pd.pivot_table(outbreaks, index='State', values='Illnesses', aggfunc='sum')\nax = df2.plot(kind='bar', color='steelblue',figsize=(25,10))\nplt.title('Foodborne Illnesses Cases By State')\nplt.ylabel('Illiness Cases')\ndisplay()"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["#Distribution of FoodBorne Illnesses by Year\nplt.cla()\ndf2 = pd.pivot_table(outbreaks, index='Year', values='Illnesses', aggfunc='sum')\nax = df2.plot(kind='bar', color='steelblue',figsize=(25,10))\nplt.title('Foodborne Illnesses Cases By Year')\nplt.ylabel('Illness Cases')\ndisplay()"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["#Distribution of illness by Months\nplt.cla()\ndf2 = pd.pivot_table(outbreaks, index='Month', values='Illnesses', aggfunc='mean')\nax = df2.plot(kind='bar', color='steelblue',figsize=(25,10))\nplt.title('Foodborne Illnesses Cases By Month')\nplt.ylabel('Illiness Cases')\ndisplay()"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["#Checking top food items\noutbreaks.Food.value_counts()"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["#Filling null values of food column with \"Unspecified\""],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["outbreaks.Food.fillna(\"Unspecified\", inplace=True)\noutbreaks.Location.fillna(\"Unknown\", inplace=True)\noutbreaks.Location.value_counts()\n"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["#Filling Hospitalizations Null value with 0\noutbreaks.Hospitalizations.fillna(0, inplace=True)"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["#Creating Normalized Column for Hospitalizations/Illnesses"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":["outbreaks['normalized_hospitalizations'] = outbreaks.apply(lambda row: round((row.Hospitalizations/row.Illnesses)*100), axis=1)"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":["#How Normalized Hospitalizations are distributed over dataset\nplt.cla()\nsns.distplot(outbreaks.normalized_hospitalizations, bins=10, color='red')\nplt.title('Distribution of Normalized Hospitalizations')\ndisplay()"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"code","source":["outbreaks.head(5)"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"code","source":["plt.cla()\nsns.distplot(outbreaks.Illnesses, bins=10, color='red')\nplt.title('Distribution of Illnesses')\ndisplay()"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"code","source":["plt.cla()\nsns.distplot(np.log10(outbreaks.Illnesses), bins=10, color='red')\nplt.title('Distribution of Illnesses standardized by log scale')\ndisplay()"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"code","source":["#Adding new column log- illness on our data"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"code","source":["outbreaks['Illnesses_log'] = np.log(outbreaks.Illnesses)"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"code","source":["outbreaks.head()"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"code","source":["# Load functionality to manipulate dataframes\nfrom pyspark.sql import functions as fn"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"code","source":["# Functionality for computing features\nfrom pyspark.ml import feature\n# Functionality for regression\nfrom pyspark.ml import regression\n# Funcionality for classification\nfrom pyspark.ml import classification\n# Object for creating sequences of transformations\nfrom pyspark.ml import Pipeline"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"code","source":["df = spark.createDataFrame(outbreaks)"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"code","source":["df.dtypes"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"code","source":["training_df, validation_df, testing_df = df.randomSplit([0.6, 0.3, 0.1])\ndisplay(training_df)"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"code","source":["#Base Model\nmodel1 = Pipeline(stages=[\n  feature.VectorAssembler(inputCols=['Year'], outputCol='features'),\n  regression.LinearRegression(featuresCol='features', labelCol='Illnesses_log')  \n]).fit(training_df)"],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"code","source":["rmse = fn.sqrt(fn.avg((fn.col('Illnesses_log') - fn.col('prediction'))**2))\nmodel1.transform(validation_df).select(rmse).show()"],"metadata":{},"outputs":[],"execution_count":37},{"cell_type":"code","source":["model1.transform(testing_df).select(rmse).show(100)"],"metadata":{},"outputs":[],"execution_count":38},{"cell_type":"code","source":["model1.transform(testing_df).select((fn.col('Illnesses_log') - fn.col('prediction'))**2, fn.col('Illnesses_log'), fn.col('prediction')).show(5000)"],"metadata":{},"outputs":[],"execution_count":39},{"cell_type":"code","source":["#Model 2 - with year, State and Month\nmodel2 = Pipeline(stages=[feature.VectorAssembler(inputCols=['Year'],\n                                        outputCol='features'),\n                          feature.StringIndexer(inputCol='Month', outputCol='encoded_Month'),\n                          feature.VectorAssembler(inputCols=['features', 'encoded_Month'], outputCol='semi_final_features'),\n                          feature.StringIndexer(inputCol='State', outputCol='encoded_State'),\n                          feature.VectorAssembler(inputCols=['semi_final_features', 'encoded_State'], outputCol='final_features'),\n                 regression.LinearRegression(featuresCol='final_features', labelCol='Illnesses_log')]).fit(training_df)"],"metadata":{},"outputs":[],"execution_count":40},{"cell_type":"code","source":["model2.transform(validation_df).select(rmse).show(100)"],"metadata":{},"outputs":[],"execution_count":41},{"cell_type":"code","source":["model2.transform(testing_df).select(rmse).show(100)"],"metadata":{},"outputs":[],"execution_count":42},{"cell_type":"code","source":["model2.transform(testing_df).select((fn.col('Illnesses_log') - fn.col('prediction'))**2, fn.col('Illnesses_log'), fn.col('prediction')).show(5000)"],"metadata":{},"outputs":[],"execution_count":43},{"cell_type":"code","source":["#Testing String Indexer\n#indexer_model = StringIndexer(inputCol='Month', outputCol=\"{0}_indexed\".format('Month')).fit(training_df)\n#indexed_df = indexer_model.transform(training_df)\n#indexed_df.show(5)"],"metadata":{},"outputs":[],"execution_count":44},{"cell_type":"code","source":["#Model 3 \n#With state, Month only\nmodel3 = Pipeline(stages=[feature.StringIndexer(inputCol='Month', outputCol='encoded_Month'),\n                          feature.VectorAssembler(inputCols=['encoded_Month'], outputCol='semi_final_features'),\n                          feature.StringIndexer(inputCol='State', outputCol='encoded_State'),\n                          feature.VectorAssembler(inputCols=['semi_final_features', 'encoded_State'], outputCol='final_features'),\n                 regression.LinearRegression(labelCol='Illnesses_log', featuresCol='final_features')]).fit(training_df)"],"metadata":{},"outputs":[],"execution_count":45},{"cell_type":"code","source":["model3.transform(validation_df).select(rmse).show()"],"metadata":{},"outputs":[],"execution_count":46},{"cell_type":"code","source":["model3.transform(testing_df).select(rmse).show()"],"metadata":{},"outputs":[],"execution_count":47},{"cell_type":"code","source":["#Model 3> Model 2 > Model 1"],"metadata":{},"outputs":[],"execution_count":48},{"cell_type":"code","source":["#Adding String Indexer\nindexer_model = StringIndexer(inputCol='Month', outputCol=\"{0}_indexed\".format('Month')).fit(df)\nindexed_df = indexer_model.transform(df)\nindexer_model2 = StringIndexer(inputCol='State', outputCol=\"{0}_indexed\".format('State')).fit(indexed_df)\nindexed_df = indexer_model2.transform(indexed_df)\nindexed_df.toPandas()"],"metadata":{},"outputs":[],"execution_count":49},{"cell_type":"code","source":["#We have a new dataframe indexed with states and month\nindexed_df.head(5)"],"metadata":{},"outputs":[],"execution_count":50},{"cell_type":"code","source":["outbreaks_new= outbreaks.copy()\noutbreaks_new.head(5)"],"metadata":{},"outputs":[],"execution_count":51},{"cell_type":"code","source":["outbreaks_new.drop(['Hospitalizations'], axis=1, inplace=True)\noutbreaks_new.head(1)"],"metadata":{},"outputs":[],"execution_count":52},{"cell_type":"code","source":["outbreaks_new.Location.value_counts()"],"metadata":{},"outputs":[],"execution_count":53},{"cell_type":"code","source":["outbreaks_new.Food.value_counts()"],"metadata":{},"outputs":[],"execution_count":54},{"cell_type":"code","source":["#Creating dummies for Location Variable\noutbreaks_new['Location_modified']=outbreaks_new['Location'].str.split(';').str[0]\noutbreaks_new['Food_modified']=outbreaks_new['Food'].str.split(',').str[0]\noutbreaks_new['Food_modified_new']=outbreaks_new['Food_modified'].str.split(';').str[0]"],"metadata":{},"outputs":[],"execution_count":55},{"cell_type":"code","source":["outbreaks_new.Food_modified_new.value_counts()"],"metadata":{},"outputs":[],"execution_count":56},{"cell_type":"code","source":["list(outbreaks_new.columns)"],"metadata":{},"outputs":[],"execution_count":57},{"cell_type":"code","source":["df = spark.createDataFrame(outbreaks_new)\ndf.show(50)"],"metadata":{},"outputs":[],"execution_count":58},{"cell_type":"code","source":["categorical_columns = [\"Year\",\"Month\",\"State\", \"Location_modified\", \"Food_modified_new\"]\nstring_indexer_models = []\none_hot_encoders = []\nfor col_name in categorical_columns:\n    # OneHotEncoders map number indices column to column of binary vectors\n    string_indexer_model = StringIndexer(inputCol=col_name, outputCol=\"{0}_indexed\".format(col_name)).fit(df)\n    df = string_indexer_model.transform(df)\n    string_indexer_models.append(string_indexer_model)\n    \n    one_hot_encoder = OneHotEncoder(inputCol=\"{0}_indexed\".format(col_name), outputCol=\"{0}_encoded\".format(col_name), dropLast=False)\n    df = one_hot_encoder.transform(df)\n    \n    one_hot_encoders.append(one_hot_encoder)"],"metadata":{},"outputs":[],"execution_count":59},{"cell_type":"code","source":["display(df)"],"metadata":{},"outputs":[],"execution_count":60},{"cell_type":"code","source":["#Correlation between all features and Illnesses_log\ncorr_columns = [\"Year\",\"Month_indexed\",\"State_indexed\", \"Location_modified_indexed\", \"Food_modified_new_indexed\", \"Illnesses_log\"]\ncorr_df=df.select(corr_columns).toPandas()\nplt.cla()\nsns.heatmap(corr_df.corr(),annot=True)\ndisplay()"],"metadata":{},"outputs":[],"execution_count":61},{"cell_type":"code","source":["training_df, validation_df, testing_df = df.randomSplit([0.6, 0.3, 0.1])\n#display(testing_df)\ntesting_df.columns"],"metadata":{},"outputs":[],"execution_count":62},{"cell_type":"code","source":["df.printSchema()"],"metadata":{},"outputs":[],"execution_count":63},{"cell_type":"code","source":["model4 = Pipeline(stages=[\n  feature.VectorAssembler(inputCols=['State_encoded', 'Location_modified_encoded'],outputCol='features'),\n  regression.LinearRegression(featuresCol='features', labelCol='Illnesses_log')]).fit(training_df)"],"metadata":{},"outputs":[],"execution_count":64},{"cell_type":"code","source":["model4.transform(validation_df).select(rmse).show()"],"metadata":{},"outputs":[],"execution_count":65},{"cell_type":"code","source":["model4.transform(testing_df).select(rmse).show()"],"metadata":{},"outputs":[],"execution_count":66},{"cell_type":"code","source":["model5 = Pipeline(stages=[\n  feature.VectorAssembler(inputCols=['State_encoded', 'Location_modified_encoded', 'Food_modified_new_encoded'],outputCol='features'),\n  regression.LinearRegression(featuresCol='features', labelCol='Illnesses_log',maxIter=5, regParam=0.0, elasticNetParam=0.0)]).fit(training_df)\n"],"metadata":{},"outputs":[],"execution_count":67},{"cell_type":"code","source":["model5.transform(training_df).select(rmse).show()"],"metadata":{},"outputs":[],"execution_count":68},{"cell_type":"code","source":["model5.transform(validation_df).select(rmse).show()"],"metadata":{},"outputs":[],"execution_count":69},{"cell_type":"code","source":["model5.transform(testing_df).select(rmse).show()"],"metadata":{},"outputs":[],"execution_count":70},{"cell_type":"code","source":["model5.transform(testing_df).select((fn.col('Illnesses_log') - fn.col('prediction'))**2, fn.col('Illnesses_log'), fn.col('prediction')).show(500)"],"metadata":{},"outputs":[],"execution_count":71},{"cell_type":"code","source":["#Model 6 with all the features - Date, Month, State, Food and Location\nmodel6 = Pipeline(stages=[\n  feature.VectorAssembler(inputCols=['Month_encoded','Year_encoded','State_encoded', 'Location_modified_encoded', 'Food_modified_new_encoded'],outputCol='features'),\n  regression.LinearRegression(featuresCol='features', labelCol='Illnesses_log',maxIter=5, regParam=0.00, elasticNetParam=0.0)]).fit(training_df)\n"],"metadata":{},"outputs":[],"execution_count":72},{"cell_type":"code","source":["model6.transform(training_df).select(rmse).show()"],"metadata":{},"outputs":[],"execution_count":73},{"cell_type":"code","source":["model6.transform(validation_df).select(rmse).show()"],"metadata":{},"outputs":[],"execution_count":74},{"cell_type":"code","source":["model6.transform(testing_df).select(rmse).show()"],"metadata":{},"outputs":[],"execution_count":75},{"cell_type":"code","source":["#Model 6 modification with non encoded by indexed values\nmodel7 = Pipeline(stages=[\n  feature.VectorAssembler(inputCols=['Month_indexed','Year_indexed','State_indexed', 'Location_modified_indexed', 'Food_modified_new_indexed'],outputCol='features'),\n  regression.LinearRegression(featuresCol='features', labelCol='Illnesses_log',maxIter=5, regParam=0.0, elasticNetParam=0.0)]).fit(training_df)\n"],"metadata":{},"outputs":[],"execution_count":76},{"cell_type":"code","source":["display(testing_df)\n"],"metadata":{},"outputs":[],"execution_count":77},{"cell_type":"code","source":["model7.transform(training_df).select(rmse).show()"],"metadata":{},"outputs":[],"execution_count":78},{"cell_type":"code","source":["model7.transform(validation_df).select(rmse).show()"],"metadata":{},"outputs":[],"execution_count":79},{"cell_type":"code","source":["model7.transform(testing_df).select(rmse).show()"],"metadata":{},"outputs":[],"execution_count":80},{"cell_type":"code","source":["#taking indexes instead of encoding might not be a great idea"],"metadata":{},"outputs":[],"execution_count":81},{"cell_type":"code","source":["#Model 5 is the best till now\\\nimport pyspark.ml.tuning as tune\ngrid = tune.ParamGridBuilder()"],"metadata":{},"outputs":[],"execution_count":82},{"cell_type":"code","source":["reg = regression.LinearRegression(labelCol = 'Illnesses_log', featuresCol = 'features', maxIter=5)"],"metadata":{},"outputs":[],"execution_count":83},{"cell_type":"code","source":["grid = grid.addGrid(reg.elasticNetParam, [0.0, 0.2, 0.4, 0.6, 0.8, 1.0])"],"metadata":{},"outputs":[],"execution_count":84},{"cell_type":"code","source":["grid = grid.addGrid(reg.regParam, np.arange(0,.1,.01))"],"metadata":{},"outputs":[],"execution_count":85},{"cell_type":"code","source":["np.arange(0,0.1,0.01)"],"metadata":{},"outputs":[],"execution_count":86},{"cell_type":"code","source":["grid = grid.build()"],"metadata":{},"outputs":[],"execution_count":87},{"cell_type":"code","source":["evaluator = RegressionEvaluator(labelCol=reg.getLabelCol(), predictionCol=reg.getPredictionCol())"],"metadata":{},"outputs":[],"execution_count":88},{"cell_type":"code","source":["va= feature.VectorAssembler(inputCols=['Month_encoded','Year_encoded','State_encoded', 'Location_modified_encoded', 'Food_modified_new_encoded'],outputCol='features')"],"metadata":{},"outputs":[],"execution_count":89},{"cell_type":"code","source":["crossPipe = Pipeline(stages=[va,reg])"],"metadata":{},"outputs":[],"execution_count":90},{"cell_type":"code","source":["cv = tune.CrossValidator(estimator = crossPipe, estimatorParamMaps = grid, evaluator= evaluator, numFolds = 3)"],"metadata":{},"outputs":[],"execution_count":91},{"cell_type":"code","source":["list1=list()"],"metadata":{},"outputs":[],"execution_count":92},{"cell_type":"code","source":["class CrossValidatorVerbose(CrossValidator):\n  def _fit(self, dataset):\n        est = self.getOrDefault(self.estimator)\n        epm = self.getOrDefault(self.estimatorParamMaps)\n        numModels = len(epm)\n\n        eva = self.getOrDefault(self.evaluator)\n        metricName = eva.getMetricName()\n\n        nFolds = self.getOrDefault(self.numFolds)\n        seed = self.getOrDefault(self.seed)\n        h = 1.0 / nFolds\n\n        randCol = self.uid + \"_rand\"\n        df = dataset.select(\"*\", rand(seed).alias(randCol))\n        metrics = [0.0] * numModels\n\n        for i in range(nFolds):\n            foldNum = i + 1\n            print(\"Comparing models on fold %d\" % foldNum)\n\n            validateLB = i * h\n            validateUB = (i + 1) * h\n            condition = (df[randCol] >= validateLB) & (df[randCol] < validateUB)\n            validation = df.filter(condition)\n            train = df.filter(~condition)\n\n            for j in range(numModels):\n                paramMap = epm[j]\n                model = est.fit(train, paramMap)\n                # TODO: duplicate evaluator to take extra params from input\n                metric = eva.evaluate(model.transform(validation, paramMap))\n                metrics[j] += metric\n\n                avgSoFar = metrics[j] / foldNum\n                print(\"params: %s\\t%s: %f\\tavg: %f\" % (\n                    {param.name: val for (param, val) in paramMap.items()},\n                    metricName, metric, avgSoFar))\n                list1.append([{param.name: val for (param, val) in paramMap.items()},metric,avgSoFar])\n              # paramsList.append([[{param.name: val for (param, val) in paramMap.items()},metricName,metric,avgSoFar]])\n\n        if eva.isLargerBetter():\n            bestIndex = np.argmax(metrics)\n        else:\n            bestIndex = np.argmin(metrics)\n\n        bestParams = epm[bestIndex]\n        bestModel = est.fit(dataset, bestParams)\n        avgMetrics = [m / nFolds for m in metrics]\n        bestAvg = avgMetrics[bestIndex]\n        print(\"Best model:\\nparams: %s\\t%s: %f\" % (\n            {param.name: val for (param, val) in bestParams.items()},\n            metricName, bestAvg))\n\n        return self._copyValues(CrossValidatorModel(bestModel, avgMetrics))\n      "],"metadata":{},"outputs":[],"execution_count":93},{"cell_type":"code","source":["cvVer = CrossValidatorVerbose(estimator = crossPipe, estimatorParamMaps = grid, evaluator= evaluator, numFolds = 3)"],"metadata":{},"outputs":[],"execution_count":94},{"cell_type":"code","source":["from pyspark.sql.functions import rand"],"metadata":{},"outputs":[],"execution_count":95},{"cell_type":"code","source":["training, test = df.randomSplit([0.7,0.3],0)"],"metadata":{},"outputs":[],"execution_count":96},{"cell_type":"code","source":["cvVer.fit(training).transform(test)"],"metadata":{},"outputs":[],"execution_count":97},{"cell_type":"code","source":["#Fetch Parameters automatically\nlist1\ndf_cross_val=pd.DataFrame(list1,columns=['Regularization_Parameters','RMSE','AVG'])\ndf_cross_val.head()\ndf_cross_val['Regularization_Parameters']=df_cross_val['Regularization_Parameters'].astype('str') "],"metadata":{},"outputs":[],"execution_count":98},{"cell_type":"code","source":["df_cross_val['Regularization_Parameters'] = df_cross_val['Regularization_Parameters'].replace({'regParam' : 'regP'}, regex=True)\ndf_cross_val['Regularization_Parameters'] = df_cross_val['Regularization_Parameters'].replace({'elasticNetParam' : 'elNetP'}, regex=True)\ndf_cross_val['Regularization_Parameters'] = df_cross_val['Regularization_Parameters'].replace({'{' : ''}, regex=True)\ndf_cross_val['Regularization_Parameters'] = df_cross_val['Regularization_Parameters'].replace({'}' : ''}, regex=True)\n"],"metadata":{},"outputs":[],"execution_count":99},{"cell_type":"code","source":["df_cross_val.head(10)"],"metadata":{},"outputs":[],"execution_count":100},{"cell_type":"code","source":["df_cross_val_best=df_cross_val.nsmallest(10, 'RMSE')\ndf_cross_val_best.head()"],"metadata":{},"outputs":[],"execution_count":101},{"cell_type":"code","source":["#Plot of regularizations with RMSE values\nplt.figure()\nfig=plt.figure(figsize=(25, 10), dpi= 60)\nsns.pointplot( y = 'RMSE', x = 'Regularization_Parameters', data = df_cross_val_best, palette='Blues',)\nplt.xticks(rotation =60)\nplt.tight_layout()\ndisplay()"],"metadata":{},"outputs":[],"execution_count":102},{"cell_type":"code","source":["a = spark.createDataFrame(df_cross_val_best)\ndisplay(a)"],"metadata":{},"outputs":[],"execution_count":103},{"cell_type":"code","source":["#Best Value of Regular Param and Elastic Param \n#params: {regParam': 0.01, 'elasticNetParam': 0.8}\trmse: 0.952868, it will change everytime\nregParamBest=0.01 \nelasticNetParamBest=0.8"],"metadata":{},"outputs":[],"execution_count":104},{"cell_type":"code","source":["#Putting these best values in the model 6 taken by us\nfinal_regression_model = Pipeline(stages=[\n  feature.VectorAssembler(inputCols=['Month_encoded','Year_encoded','State_encoded','Location_modified_encoded','Food_modified_new_encoded'],outputCol='features'),\n  regression.LinearRegression(featuresCol='features', labelCol='Illnesses_log',maxIter=5, regParam=regParamBest, elasticNetParam=elasticNetParamBest)]).fit(training)"],"metadata":{},"outputs":[],"execution_count":105},{"cell_type":"code","source":["final_regression_model.transform(training).select(rmse).show()\n#older value was 0.8996500501049035"],"metadata":{},"outputs":[],"execution_count":106},{"cell_type":"code","source":["\ntemp_result=final_regression_model.transform(test)\ndisplay(temp_result)"],"metadata":{},"outputs":[],"execution_count":107},{"cell_type":"code","source":["#Plotting Linear Model\nplt.cla()\nplotting_df=temp_result.toPandas()\nplt.scatter(plotting_df['Illnesses_log'],plotting_df[\"prediction\"])\ndisplay()"],"metadata":{},"outputs":[],"execution_count":108},{"cell_type":"code","source":["training_df.printSchema()"],"metadata":{},"outputs":[],"execution_count":109},{"cell_type":"code","source":["finalModelFit =  cv.fit(training)"],"metadata":{},"outputs":[],"execution_count":110},{"cell_type":"code","source":["final_RMSE_value = evaluator.evaluate(finalModelFit.transform(test))"],"metadata":{},"outputs":[],"execution_count":111},{"cell_type":"code","source":["final_RMSE_value"],"metadata":{},"outputs":[],"execution_count":112},{"cell_type":"code","source":["pred =  finalModelFit.transform(test)"],"metadata":{},"outputs":[],"execution_count":113},{"cell_type":"code","source":["pred.select('Illnesses_log', 'prediction').show(500)"],"metadata":{},"outputs":[],"execution_count":114},{"cell_type":"code","source":["final_regression_model.stages[-1].coefficients"],"metadata":{},"outputs":[],"execution_count":115},{"cell_type":"code","source":["final_regression_model.stages[-1].summary.r2"],"metadata":{},"outputs":[],"execution_count":116},{"cell_type":"code","source":["#Evaluating Best Model - Model 6 - final regression model\n#Finding \"year\" coefficients \n\ncoefficients_list=final_regression_model.stages[-1].coefficients\ncoefficients_list"],"metadata":{},"outputs":[],"execution_count":117},{"cell_type":"code","source":["year_list=df.toPandas()['Year'].unique()\nlen(year_list)\nyear_coefficients_list= coefficients_list[:18]\nyear_coefficients_list\nyear_coefficients = pd.DataFrame(\n    {'year': year_list,\n     'coefficients': year_coefficients_list\n    })\nyear_coefficients.head(19)"],"metadata":{},"outputs":[],"execution_count":118},{"cell_type":"code","source":["#Plotting year coefficients\nplt.cla()\nfig=plt.figure(figsize=(25, 10), dpi= 80)\nsns.barplot( y = 'coefficients', x = 'year', data = year_coefficients, palette='Blues')\nplt.xticks(rotation = 60)\nplt.tight_layout()\ndisplay()"],"metadata":{},"outputs":[],"execution_count":119},{"cell_type":"code","source":["# Year 2000, Year 2013, Year 2014 are most important factors here"],"metadata":{},"outputs":[],"execution_count":120},{"cell_type":"code","source":["#Month coefficients\nmonth_list=df.toPandas()['Month_indexed'].unique()\nmonth_name_list=df.toPandas()['Month'].unique()\nlen(month_list)\nmonth_coefficients_list= coefficients_list[18:30]\nmonth_coefficients_list\nmonth_coefficients = pd.DataFrame(\n    {'month': month_list,\n     'coefficients': month_coefficients_list,\n     'month_name':month_name_list\n    })\nmonth_coefficients.head(13)\n"],"metadata":{},"outputs":[],"execution_count":121},{"cell_type":"code","source":["#Plotting month coefficients\nplt.cla()\nfig=plt.figure(figsize=(25, 10), dpi= 80)\nsns.barplot( y = 'coefficients', x = 'month_name', data = month_coefficients, palette='GnBu_d')\nplt.xticks(rotation = 60)\nplt.tight_layout()\ndisplay()"],"metadata":{},"outputs":[],"execution_count":122},{"cell_type":"code","source":["#Conclusion for Months\n#Jan seems to be an important one in terms of months"],"metadata":{},"outputs":[],"execution_count":123},{"cell_type":"code","source":["#For State, coffecients\nstate_list=df.toPandas()['State_indexed'].unique()\nstate_name_list=df.toPandas()['State'].unique()\nlen(state_list)\n\n"],"metadata":{},"outputs":[],"execution_count":124},{"cell_type":"code","source":["state_coefficients_list= coefficients_list[30:85]\nstate_coefficients_list\nstate_coefficients = pd.DataFrame(\n    {'state': state_list,\n     'coefficients': state_coefficients_list,\n     'state_name':state_name_list\n    })\nstate_coefficients_temp=state_coefficients.nlargest(10, 'coefficients')"],"metadata":{},"outputs":[],"execution_count":125},{"cell_type":"code","source":["plt.cla()\nfig=plt.figure(figsize=(25, 10), dpi= 80)\nsns.barplot( y = 'coefficients', x = 'state_name', data = state_coefficients_temp, palette='coolwarm')\nplt.xticks(rotation = 60)\nplt.tight_layout()\ndisplay()"],"metadata":{},"outputs":[],"execution_count":126},{"cell_type":"code","source":["#Importance of top 10 states in predicting results shown above"],"metadata":{},"outputs":[],"execution_count":127},{"cell_type":"code","source":["#Based on Location, top most features\nlocation_list=df.toPandas()['Location_modified_indexed'].unique()\nlocation_name_list=df.toPandas()['Location_modified'].unique()\nlen(location_list)\n\n"],"metadata":{},"outputs":[],"execution_count":128},{"cell_type":"code","source":["location_coefficients_list= coefficients_list[85:106]\nlocation_coefficients_list\nlocation_coefficients = pd.DataFrame(\n    {'location': location_list,\n     'coefficients': location_coefficients_list,\n     'location_name':location_name_list\n    })\nlocation_coefficients.head(13)"],"metadata":{},"outputs":[],"execution_count":129},{"cell_type":"code","source":["#Plotting year coefficients\nplt.cla()\nfig=plt.figure(figsize=(25, 10), dpi= 100)\nsns.barplot( y = 'coefficients', x = 'location_name', data = location_coefficients, palette='Blues')\nplt.xticks(rotation = 90)\nplt.tight_layout()\ndisplay()"],"metadata":{},"outputs":[],"execution_count":130},{"cell_type":"code","source":["#Important features found here like food consumed at hospital , or at assisted living facility  or school/college/University"],"metadata":{},"outputs":[],"execution_count":131},{"cell_type":"code","source":["#Last but not least food\nfood_list=df.toPandas()['Food_modified_new_indexed'].unique()\nfood_name_list=df.toPandas()['Food_modified_new'].unique()\nlen(food_list)"],"metadata":{},"outputs":[],"execution_count":132},{"cell_type":"code","source":["food_coefficients_list= coefficients_list[106:1055]\nfood_coefficients_list\nfood_coefficients = pd.DataFrame(\n    {'food': food_list,\n     'coefficients': food_coefficients_list,\n     'food_name':food_name_list\n    })\nfood_coefficients_temp=food_coefficients.nlargest(20, 'coefficients')"],"metadata":{},"outputs":[],"execution_count":133},{"cell_type":"code","source":["#Plotting year coefficients\nplt.figure()\nfig=plt.figure(figsize=(25, 10), dpi= 80)\nsns.barplot( y = 'coefficients', x = 'food_name', data = food_coefficients_temp, palette='husl')\nplt.xticks(rotation = 60)\nplt.tight_layout()\ndisplay()"],"metadata":{},"outputs":[],"execution_count":134},{"cell_type":"code","source":["#Main Food Items involved"],"metadata":{},"outputs":[],"execution_count":135},{"cell_type":"code","source":["#reg_best = regression.LinearRegression(labelCol = 'Illnesses_log', featuresCol = 'features', maxIter=5, regParam=0.040000000000000001, elasticNetParam=0.2)"],"metadata":{},"outputs":[],"execution_count":136},{"cell_type":"code","source":["df_for_prof.dtypes"],"metadata":{},"outputs":[],"execution_count":137},{"cell_type":"code","source":["df_for_prof = spark.createDataFrame(outbreaks_new)\ndf_for_prof.show(50)\ncategorical_columns = [\"Year\",\"Month\",\"State\", \"Location_modified\", \"Food_modified_new\"]\nstring_indexer_models = []\none_hot_encoders = []\ntraining_df_prof, validation_df_prof, testing_df_prof = df_for_prof.randomSplit([0.6, 0.3, 0.1])\n#display(testing_df)\ndisplay(testing_df_prof)\ntest_df_prof=testing_df_prof.toPandas()\ntest_df_prof.loc[-1] = [2003, \"August\", \"Utah\", \"Restaurant\", \"Lo Mein\", 4, 3, 0.111, \"Restaurant\", \"Lo Mein\", \"Lo Mein\" ]  # adding a row\ntest_df_prof.index = test_df_prof.index + 1  # shifting index\ntest_df_prof = test_df_prof.sort_index()"],"metadata":{},"outputs":[],"execution_count":138},{"cell_type":"code","source":["testing_df_prof=spark.createDataFrame(test_df_prof)\ndisplay(testing_df_prof)"],"metadata":{},"outputs":[],"execution_count":139},{"cell_type":"code","source":["for col_name in categorical_columns:\n    # OneHotEncoders map number indices column to column of binary vectors\n    string_indexer_model = StringIndexer(inputCol=col_name, outputCol=\"{0}_indexed\".format(col_name)).fit(testing_df_prof)\n    testing_df_prof = string_indexer_model.transform(testing_df_prof)\n    string_indexer_models.append(string_indexer_model)\n    \n    one_hot_encoder = OneHotEncoder(inputCol=\"{0}_indexed\".format(col_name), outputCol=\"{0}_encoded\".format(col_name), dropLast=False)\n    testing_df_prof = one_hot_encoder.transform(testing_df_prof)\n    \n    one_hot_encoders.append(one_hot_encoder)\ndisplay(testing_df_prof)"],"metadata":{},"outputs":[],"execution_count":140},{"cell_type":"code","source":["#model6 = Pipeline(stages=[\n#  feature.VectorAssembler(inputCols=['Month_encoded','Year_encoded','State_encoded', 'Location_modified_encoded', 'Food_modified_new_encoded'],outputCol='features'),\n#  regression.LinearRegression(featuresCol='features', labelCol='Illnesses_log',maxIter=5, regParam=0.00, elasticNetParam=0.0)]).fit(training_df)#\nmodel6.transform(testing_df_prof).select(fn.col('prediction')).show(5)\ntraining_df.dtypes"],"metadata":{},"outputs":[],"execution_count":141},{"cell_type":"code","source":["training_df.dtypes"],"metadata":{},"outputs":[],"execution_count":142},{"cell_type":"code","source":["testing_df_prof.dtypes"],"metadata":{},"outputs":[],"execution_count":143},{"cell_type":"code","source":["model8 = Pipeline(stages=[\n  va,\n  regression.GeneralizedLinearRegression(family=\"gaussian\", link=\"identity\",featuresCol='features', labelCol='Illnesses_log',  maxIter=5, regParam=0.0 )]).fit(training_df)"],"metadata":{},"outputs":[],"execution_count":144},{"cell_type":"code","source":["model8.transform(validation_df).select(rmse).show()"],"metadata":{},"outputs":[],"execution_count":145},{"cell_type":"code","source":["model8.transform(testing_df).select(rmse).show()"],"metadata":{},"outputs":[],"execution_count":146},{"cell_type":"code","source":["training_df.printSchema()"],"metadata":{},"outputs":[],"execution_count":147},{"cell_type":"code","source":["display(df)"],"metadata":{},"outputs":[],"execution_count":148},{"cell_type":"code","source":["#Decision Tree Regression\nmodel9 = Pipeline(stages=[\nva,\nregression.DecisionTreeRegressor(featuresCol='features', labelCol='Illnesses_log')]).fit(training_df)"],"metadata":{},"outputs":[],"execution_count":149},{"cell_type":"code","source":["model9.transform(validation_df).select(rmse).show()"],"metadata":{},"outputs":[],"execution_count":150},{"cell_type":"code","source":["model9.transform(testing_df).select(rmse).show()"],"metadata":{},"outputs":[],"execution_count":151},{"cell_type":"code","source":["#Random Forest regression\nmodel10 = Pipeline(stages=[\nva,\nregression.RandomForestRegressor(featuresCol='features', labelCol='Illnesses_log')]).fit(training_df)"],"metadata":{},"outputs":[],"execution_count":152},{"cell_type":"code","source":["model10.transform(validation_df).select(rmse).show()"],"metadata":{},"outputs":[],"execution_count":153},{"cell_type":"code","source":["model10.transform(testing_df).select(rmse).show()"],"metadata":{},"outputs":[],"execution_count":154},{"cell_type":"code","source":["#Gradient Boosting Regression\nmodel11 = Pipeline(stages=[\nva,\nregression.GBTRegressor(featuresCol='features', labelCol='Illnesses_log')]).fit(training_df)"],"metadata":{},"outputs":[],"execution_count":155},{"cell_type":"code","source":["model11.transform(validation_df).select(rmse).show()"],"metadata":{},"outputs":[],"execution_count":156},{"cell_type":"code","source":["model11.transform(testing_df).select(rmse).show()"],"metadata":{},"outputs":[],"execution_count":157},{"cell_type":"code","source":["#The best Model has been found with linear regression with set elastic and normal regularization parameters - Model 6"],"metadata":{},"outputs":[],"execution_count":158},{"cell_type":"code","source":["#Trying Time Series Forecasting\noutbreaks_time_series = outbreaks_new.copy()"],"metadata":{},"outputs":[],"execution_count":159},{"cell_type":"code","source":["outbreaks_time_series.head()"],"metadata":{},"outputs":[],"execution_count":160},{"cell_type":"code","source":["import calendar\nd = {'January':'01', 'February':'02', 'March':'03', 'April':'04','May':'05', 'June':'06', 'July':'07', 'August':'08', 'Spetember':'09','October':'10', 'November':'11', 'December':'12' }"],"metadata":{},"outputs":[],"execution_count":161},{"cell_type":"code","source":["outbreaks_time_series.Month = outbreaks_time_series.Month.map(d)"],"metadata":{},"outputs":[],"execution_count":162},{"cell_type":"code","source":["outbreaks_time_series.head(5)\noutbreaks_time_series.Month.value_counts()"],"metadata":{},"outputs":[],"execution_count":163},{"cell_type":"code","source":["outbreaks_time_series.Year= outbreaks_time_series[\"Year\"].map(str)+ \"-\" + outbreaks_time_series[\"Month\"]"],"metadata":{},"outputs":[],"execution_count":164},{"cell_type":"code","source":["outbreaks_time_series.dtypes"],"metadata":{},"outputs":[],"execution_count":165},{"cell_type":"code","source":["outbreaks_time_series.Year= outbreaks_time_series[\"Year\"]+ \"-\" + \"01\"\n"],"metadata":{},"outputs":[],"execution_count":166},{"cell_type":"code","source":["outbreaks_time_series.Year=outbreaks_time_series['Year']"],"metadata":{},"outputs":[],"execution_count":167},{"cell_type":"code","source":["outbreaks_time_series.head()"],"metadata":{},"outputs":[],"execution_count":168},{"cell_type":"code","source":["outbreaks_time_series['Year']=pd.to_datetime(outbreaks_time_series.Year, format=\"%Y-%m-%d\")"],"metadata":{},"outputs":[],"execution_count":169},{"cell_type":"code","source":["time_series_model_df=pd.DataFrame(outbreaks_time_series.groupby('Year')['Illnesses'].sum()).copy()"],"metadata":{},"outputs":[],"execution_count":170},{"cell_type":"code","source":["time_series_model_df.index"],"metadata":{},"outputs":[],"execution_count":171},{"cell_type":"code","source":["plt.cla()\ntime_series_model_df.plot(figsize=(20,10), linewidth=5, fontsize=20)\nplt.xlabel('Year', fontsize=20)\ndisplay()"],"metadata":{},"outputs":[],"execution_count":172},{"cell_type":"code","source":["#Rolling Average\nplt.cla()\nillnesses = time_series_model_df[['Illnesses']]\nillnesses.rolling(12).mean().plot(figsize=(20,10), linewidth=5, fontsize=20)\nplt.xlabel('Year', fontsize=20)\ndisplay()"],"metadata":{},"outputs":[],"execution_count":173},{"cell_type":"code","source":["time_series_analysis_df=illnesses.rolling(12).mean().reset_index()\ntime_series_analysis_df = time_series_analysis_df[np.isfinite(time_series_analysis_df['Illnesses'])]\ntime_series_analysis_df.reset_index(inplace=True)\ntime_series_analysis_df.drop(['index'], axis=1,inplace=True)\ntime_series_analysis_df.reset_index(inplace=True)\ntime_series_analysis_df"],"metadata":{},"outputs":[],"execution_count":174},{"cell_type":"code","source":["time_series_analysis_df.count()"],"metadata":{},"outputs":[],"execution_count":175},{"cell_type":"code","source":["df_time_series = spark.createDataFrame(time_series_analysis_df)\ndisplay(df_time_series)\ndf_time_series.describe\n"],"metadata":{},"outputs":[],"execution_count":176},{"cell_type":"code","source":["import pyspark.sql.functions as fn\nfrom pyspark.sql.types import *\ndf_time_series= df_time_series.select(fn.unix_timestamp(fn.col('Year'), format='yyyy-MM-dd HH:mm:ss.000').alias('date'),'index', 'Illnesses')\n"],"metadata":{},"outputs":[],"execution_count":177},{"cell_type":"code","source":["df_time_series.show()"],"metadata":{},"outputs":[],"execution_count":178},{"cell_type":"code","source":["training = df_time_series.where((df_time_series['index'] >= 0) & (df_time_series['index']<150))\ndisplay(training)"],"metadata":{},"outputs":[],"execution_count":179},{"cell_type":"code","source":["testing = df_time_series.where((df_time_series['index'] >= 150) & (df_time_series['index']<=187))\ndisplay(testing)"],"metadata":{},"outputs":[],"execution_count":180},{"cell_type":"code","source":["#training, testing = df_time_series.randomSplit([0.8, 0.2#], 0.0)"],"metadata":{},"outputs":[],"execution_count":181},{"cell_type":"code","source":["model_time = Pipeline(stages=[\n  feature.VectorAssembler(inputCols=['date'], outputCol='features'),\n  regression.LinearRegression(featuresCol='features', labelCol='Illnesses',maxIter=5, regParam=0.01, elasticNetParam=0.2)  \n]).fit(training)"],"metadata":{},"outputs":[],"execution_count":182},{"cell_type":"code","source":["rmse_time = fn.sqrt(fn.avg((fn.col('Illnesses') - fn.col('prediction'))**2))\ntest_model_time=model_time.transform(testing).select(((fn.col('Illnesses') - fn.col('prediction'))**2),fn.col('Illnesses'), fn.col('prediction'), fn.col('date'))"],"metadata":{},"outputs":[],"execution_count":183},{"cell_type":"code","source":["test_model_time.show(30)"],"metadata":{},"outputs":[],"execution_count":184},{"cell_type":"code","source":["model_time.transform(testing).select(rmse_time).show()"],"metadata":{},"outputs":[],"execution_count":185},{"cell_type":"code","source":["plt.cla()\na4_dims = (11.7, 8.27)\nfig, ax = plt.subplots(figsize=a4_dims)\ng=sns.FacetGrid(data=df_time_series.toPandas(),size=8) #mapping maps in the grids using facetgrid\ng.map(plt.scatter, 'date', 'Illnesses')\ndisplay()"],"metadata":{},"outputs":[],"execution_count":186},{"cell_type":"code","source":["test_model_df=test_model_time.toPandas()"],"metadata":{},"outputs":[],"execution_count":187},{"cell_type":"code","source":["test_model_df.count()"],"metadata":{},"outputs":[],"execution_count":188},{"cell_type":"code","source":["plt.cla()\nplt.plot(test_model_df.date, test_model_df.Illnesses, color='g')\nplt.plot(test_model_df.date, test_model_df.prediction, color='orange')\nplt.xlabel('Years 1998-2015')\nplt.ylabel('Illnesses Occurred')\nplt.title('Illnesses vs Years - Linear Regression Basic')\nplt.show()\ndisplay()"],"metadata":{},"outputs":[],"execution_count":189},{"cell_type":"code","source":["#Logistic Modeling\nmodel_time.transform(testing).select(((fn.col('Illnesses') - fn.col('prediction'))**2)).show(200)"],"metadata":{},"outputs":[],"execution_count":190},{"cell_type":"code","source":["display(df)"],"metadata":{},"outputs":[],"execution_count":191},{"cell_type":"code","source":["df.select(fn.avg(\"Illnesses_log\")).show()"],"metadata":{},"outputs":[],"execution_count":192},{"cell_type":"code","source":["df.select(fn.max(\"Illnesses_log\")).show()"],"metadata":{},"outputs":[],"execution_count":193},{"cell_type":"code","source":["df.select(fn.min(\"Illnesses_log\")).show()"],"metadata":{},"outputs":[],"execution_count":194},{"cell_type":"code","source":["outbreaks_pandas_df=df.toPandas()"],"metadata":{},"outputs":[],"execution_count":195},{"cell_type":"code","source":["outbreaks_pandas_df.head()"],"metadata":{},"outputs":[],"execution_count":196},{"cell_type":"code","source":["#Check Distribution of Illnesses_log\nplt.cla()\nsns.distplot(outbreaks_pandas_df['Illnesses_log'], kde=False, bins=30)\ndisplay()"],"metadata":{},"outputs":[],"execution_count":197},{"cell_type":"code","source":["#jointplot between Illnesses count and Illnesses_log\nplt.cla()\nsns.jointplot(x='Illnesses', y='Illnesses_log', data=outbreaks_pandas_df , kind='kde')\ndisplay()"],"metadata":{},"outputs":[],"execution_count":198},{"cell_type":"code","source":["print outbreaks_pandas_df[outbreaks_pandas_df.Illnesses_log >=3]['Illnesses']"],"metadata":{},"outputs":[],"execution_count":199},{"cell_type":"code","source":["print outbreaks_pandas_df[outbreaks_pandas_df.Illnesses_log >=2].count()"],"metadata":{},"outputs":[],"execution_count":200},{"cell_type":"code","source":["outbreaks_pandas_df['Illnesses_impact'] = np.where(outbreaks_pandas_df['Illnesses_log']>=2, 1, 0)"],"metadata":{},"outputs":[],"execution_count":201},{"cell_type":"code","source":["outbreaks_pandas_df.head()"],"metadata":{},"outputs":[],"execution_count":202},{"cell_type":"code","source":["from pyspark_pipes import pipe"],"metadata":{},"outputs":[],"execution_count":203},{"cell_type":"code","source":["df_log = spark.createDataFrame(outbreaks_pandas_df)\ndisplay(df_log)"],"metadata":{},"outputs":[],"execution_count":204},{"cell_type":"code","source":["training_df2,testing_df2 = df_log.randomSplit([0.8, 0.2], 0)\ndisplay(training_df2)"],"metadata":{},"outputs":[],"execution_count":205},{"cell_type":"code","source":["model_class1 = pipe(feature.VectorAssembler(inputCols=['Month_encoded','Year_encoded','State_encoded', 'Location_modified_encoded', 'Food_modified_new_encoded'],outputCol='features'),\n     classification.LogisticRegression(labelCol='Illnesses_impact'))"],"metadata":{},"outputs":[],"execution_count":206},{"cell_type":"code","source":["model_class1_fitted = model_class1.fit(training_df2)"],"metadata":{},"outputs":[],"execution_count":207},{"cell_type":"code","source":["model_class1_fitted.transform(testing_df2)"],"metadata":{},"outputs":[],"execution_count":208},{"cell_type":"code","source":["def binary_evaluation(model_pipeline, model_fitted, data):\n  return BinaryClassificationEvaluator(labelCol=model_pipeline.getStages()[-1].getLabelCol(), \n                                rawPredictionCol=model_pipeline.getStages()[-1].getRawPredictionCol()).\\\n    evaluate(model_fitted.transform(data))"],"metadata":{},"outputs":[],"execution_count":209},{"cell_type":"code","source":["model1ROC_test=binary_evaluation(model_class1,model_class1_fitted,testing_df2)\nmodel1ROC_test\n#base accuracy 0.7563"],"metadata":{},"outputs":[],"execution_count":210},{"cell_type":"code","source":["model1ROC_train=binary_evaluation(model_class1,model_class1_fitted,training_df2)\nmodel1ROC_train"],"metadata":{},"outputs":[],"execution_count":211},{"cell_type":"code","source":["#Use of feature scaling to see if roc increases\nmodel_class2 = pipe(feature.VectorAssembler(inputCols=['Month_encoded','Year_encoded','State_encoded', 'Location_modified_encoded', 'Food_modified_new_encoded'],outputCol='features'),\n                    feature.StandardScaler(withMean=True),\n     classification.LogisticRegression(labelCol='Illnesses_impact'))\n"],"metadata":{},"outputs":[],"execution_count":212},{"cell_type":"code","source":["model_class2_fitted = model_class2.fit(training_df2)"],"metadata":{},"outputs":[],"execution_count":213},{"cell_type":"code","source":["model_class2_fitted.transform(testing_df2)"],"metadata":{},"outputs":[],"execution_count":214},{"cell_type":"code","source":["model2ROC_train=binary_evaluation(model_class2,model_class2_fitted,training_df2)\nmodel2ROC_train\n#Scaling doesn't help much here"],"metadata":{},"outputs":[],"execution_count":215},{"cell_type":"code","source":["\nmodel2ROC_test=binary_evaluation(model_class2,model_class2_fitted,testing_df2)\nmodel2ROC_test"],"metadata":{},"outputs":[],"execution_count":216},{"cell_type":"code","source":["#Adding Regularization and Cross Validation"],"metadata":{},"outputs":[],"execution_count":217},{"cell_type":"code","source":["lr = classification.LogisticRegression(labelCol='Illnesses_impact', featuresCol = 'features', maxIter=5)\nlr.getPredictionCol()"],"metadata":{},"outputs":[],"execution_count":218},{"cell_type":"code","source":["\nparamGrid = ParamGridBuilder() \\\n    .addGrid(lr.elasticNetParam, [0., 0.2, 0.4, 0.6, 0.8, 1.0]) \\\n    .addGrid(lr.regParam, [ 0. ,  0.01,  0.02,  0.03,  0.04,  0.05,  0.06,  0.07,  0.08,  0.09]) \\\n    .build()"],"metadata":{},"outputs":[],"execution_count":219},{"cell_type":"code","source":["evaluator2 = BinaryClassificationEvaluator(labelCol=lr.getLabelCol(), rawPredictionCol=lr.getPredictionCol())\ncrossPipe2 = Pipeline(stages=[va,lr])"],"metadata":{},"outputs":[],"execution_count":220},{"cell_type":"code","source":["cv2 = tune.CrossValidator(estimator = crossPipe2, estimatorParamMaps = paramGrid, evaluator= evaluator2, numFolds = 2)"],"metadata":{},"outputs":[],"execution_count":221},{"cell_type":"code","source":["final_class_model_fitted = cv2.fit(training_df2)"],"metadata":{},"outputs":[],"execution_count":222},{"cell_type":"code","source":["model3ROC_test=evaluator2.evaluate(final_class_model_fitted.transform(testing_df2))\nmodel3ROC_test"],"metadata":{},"outputs":[],"execution_count":223},{"cell_type":"code","source":["model3ROC_train=evaluator2.evaluate(final_class_model_fitted.transform(training_df2))\nmodel3ROC_train"],"metadata":{},"outputs":[],"execution_count":224},{"cell_type":"code","source":["#Attempt\ntraining_df3, validation_df3, testing_df3 = df_log.randomSplit([0.6, 0.3, 0.1])\ndisplay(training_df3)\nfeature_assembler = VectorAssembler(inputCols=['Month_encoded','Year_encoded','State_encoded', 'Location_modified_encoded', 'Food_modified_new_encoded'], outputCol=\"features\")\nassembled_train_df = feature_assembler.transform(training_df3).cache()\n\n"],"metadata":{},"outputs":[],"execution_count":225},{"cell_type":"code","source":["assembled_validation_df = feature_assembler.transform(validation_df3).cache()"],"metadata":{},"outputs":[],"execution_count":226},{"cell_type":"code","source":["assembled_test_df = feature_assembler.transform(testing_df3).cache()"],"metadata":{},"outputs":[],"execution_count":227},{"cell_type":"code","source":["assembled_train_df.columns"],"metadata":{},"outputs":[],"execution_count":228},{"cell_type":"code","source":["# watch how we specify the class_weight using the weightCol feature\nlog_reg = LogisticRegression(featuresCol='features', labelCol='Illnesses_impact', maxIter=20, family='binomial')\nevaluator = BinaryClassificationEvaluator(rawPredictionCol='rawPrediction', labelCol='Illnesses_impact', metricName='areaUnderROC')"],"metadata":{},"outputs":[],"execution_count":229},{"cell_type":"code","source":["model = log_reg.fit(assembled_train_df)"],"metadata":{},"outputs":[],"execution_count":230},{"cell_type":"code","source":["train_preds = model.transform(assembled_validation_df)"],"metadata":{},"outputs":[],"execution_count":231},{"cell_type":"code","source":["print(train_preds.columns)"],"metadata":{},"outputs":[],"execution_count":232},{"cell_type":"code","source":["train_areaUnderROC = evaluator.evaluate(train_preds)\ntrain_areaUnderROC"],"metadata":{},"outputs":[],"execution_count":233},{"cell_type":"code","source":["trainpredlbls = train_preds.select(\"prediction\", \"Illnesses_impact\").cache()"],"metadata":{},"outputs":[],"execution_count":234},{"cell_type":"code","source":["trainpredlbls.limit(500).toPandas()"],"metadata":{},"outputs":[],"execution_count":235},{"cell_type":"code","source":["def accuracy(predlbls):\n    counttotal = predlbls.count()\n    correct = predlbls.filter(col('Illnesses_impact') == col(\"prediction\")).count()\n    wrong = predlbls.filter(col('Illnesses_impact') != col(\"prediction\")).count()\n    ratioCorrect = float(correct)/counttotal\n    print(\"Correct: {0}, Wrong: {1}, Model Accuracy: {2}\".format(correct, wrong, np.round(ratioCorrect, 2)))"],"metadata":{},"outputs":[],"execution_count":236},{"cell_type":"code","source":["accuracy(trainpredlbls)"],"metadata":{},"outputs":[],"execution_count":237},{"cell_type":"code","source":["train_summary = model.evaluate(assembled_train_df)\nvalidation_summary = model.evaluate(assembled_validation_df)"],"metadata":{},"outputs":[],"execution_count":238},{"cell_type":"code","source":["print('Training Accuracy   :', train_summary.accuracy)\nprint('Validation Accuracy :', validation_summary.accuracy)"],"metadata":{},"outputs":[],"execution_count":239},{"cell_type":"code","source":["train_summary.areaUnderROC\n"],"metadata":{},"outputs":[],"execution_count":240},{"cell_type":"code","source":["validation_summary.areaUnderROC"],"metadata":{},"outputs":[],"execution_count":241},{"cell_type":"code","source":["validation_summary.fMeasureByLabel(beta=1.0)"],"metadata":{},"outputs":[],"execution_count":242},{"cell_type":"code","source":["validation_summary.precisionByLabel"],"metadata":{},"outputs":[],"execution_count":243},{"cell_type":"code","source":["testing_summary.recallByLabel"],"metadata":{},"outputs":[],"execution_count":244},{"cell_type":"code","source":["#Our model should at least perform better than the Null Accuracy. Null Accuracy is defined as the accuracy we would have got if we would have blindly predicted the majority class of the training set as the label"],"metadata":{},"outputs":[],"execution_count":245},{"cell_type":"code","source":["#Looking for Base Model, Null Accuracy\ntrain_total = trainpredlbls.count()\ntrain_label0count = float(trainpredlbls.filter(col(\"Illnesses_impact\") == 0.0).count())\ntrain_label1count = float(trainpredlbls.filter(col(\"Illnesses_impact\") == 1.0).count())"],"metadata":{},"outputs":[],"execution_count":246},{"cell_type":"code","source":["# If we would have predicted everything to be the majority label then what would have been the accuracy\nmax(train_label0count, train_label1count) / train_total\n"],"metadata":{},"outputs":[],"execution_count":247},{"cell_type":"code","source":["#Test Accuracy\ntest_preds = model.transform(assembled_test_df)"],"metadata":{},"outputs":[],"execution_count":248},{"cell_type":"code","source":["test_areaUnderROC = evaluator.evaluate(test_preds)\ntest_areaUnderROC"],"metadata":{},"outputs":[],"execution_count":249},{"cell_type":"code","source":["testpredlbls = test_preds.select(\"prediction\", \"Illnesses_impact\")"],"metadata":{},"outputs":[],"execution_count":250},{"cell_type":"code","source":["accuracy(testpredlbls)"],"metadata":{},"outputs":[],"execution_count":251},{"cell_type":"code","source":["test_summary = model.evaluate(assembled_test_df)"],"metadata":{},"outputs":[],"execution_count":252},{"cell_type":"code","source":["test_summary.accuracy"],"metadata":{},"outputs":[],"execution_count":253},{"cell_type":"code","source":["test_summary.areaUnderROC"],"metadata":{},"outputs":[],"execution_count":254},{"cell_type":"code","source":["test_summary.fMeasureByLabel(beta=1.0)"],"metadata":{},"outputs":[],"execution_count":255},{"cell_type":"code","source":["test_summary.precisionByLabel"],"metadata":{},"outputs":[],"execution_count":256},{"cell_type":"code","source":["test_summary.recallByLabel"],"metadata":{},"outputs":[],"execution_count":257},{"cell_type":"code","source":["test_summary.roc.limit(10).toPandas()"],"metadata":{},"outputs":[],"execution_count":258},{"cell_type":"code","source":["train_roc_pdf = train_summary.roc.toPandas()\nvalidation_roc_pdf = validation_summary.roc.toPandas()\ntest_roc_pdf = test_summary.roc.toPandas()"],"metadata":{},"outputs":[],"execution_count":259},{"cell_type":"code","source":["plt.figure(figsize=(6,4))\nplt.plot(train_roc_pdf['FPR'], train_roc_pdf['TPR'], lw=1, label='Train AUC = %0.2f' % (train_summary.areaUnderROC))\nplt.plot(validation_roc_pdf['FPR'], validation_roc_pdf['TPR'], lw=1, label='Validation AUC = %0.2f' % (test_summary.areaUnderROC))\nplt.plot(test_roc_pdf['FPR'], test_roc_pdf['TPR'], lw=1, label='Test AUC = %0.2f' % (validation_summary.areaUnderROC))\nplt.plot([0, 1], [0, 1], '--', color=(0.6, 0.6, 0.6), label='NULL Accuracy')\nplt.title('ROC AUC Curve')\nplt.tight_layout()\nplt.legend(loc=\"best\" )\ndisplay()"],"metadata":{},"outputs":[],"execution_count":260},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":261}],"metadata":{"name":"Project_Work","notebookId":3230718606882710},"nbformat":4,"nbformat_minor":0}
